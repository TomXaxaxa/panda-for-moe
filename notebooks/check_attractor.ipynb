{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from dystformer.utils import (\n",
    "    load_trajectory_from_arrow,\n",
    "    plot_trajs_multivariate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_name = \"improved/final_skew40/train\"\n",
    "system_name = \"RikitakeDynamo_LuChen\"\n",
    "\n",
    "# split_name = \"final_base40/train\"\n",
    "# system_name = \"Tsucs2\"\n",
    "\n",
    "# split_name = \"big_base80_run1/train\"\n",
    "# system_name = \"Aizawa\"\n",
    "\n",
    "# split_name = \"final_base20/train\"\n",
    "# system_name = \"Lorenz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(DATA_DIR, split_name, system_name, \"1_T-4096.arrow\")\n",
    "dyst_coords, _ = load_trajectory_from_arrow(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyst_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajs_multivariate(\n",
    "    np.expand_dims(dyst_coords, axis=0),\n",
    "    plot_name=f\"{system_name}\",\n",
    "    show_plot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = dyst_coords.shape[0]\n",
    "for i in range(dim):\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.plot(dyst_coords[i], \"b-\", color=\"tab:blue\")\n",
    "    plt.title(f\"Dimension {i}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample trajectory.\n",
    "# Here we generate a simple (regular) trajectory as an example.\n",
    "t = np.linspace(0, 10 * np.pi, 4096)\n",
    "\n",
    "test_system_periodic = np.array(\n",
    "    [\n",
    "        np.sin(t),  # x-coordinate\n",
    "        np.sin(2 * t),  # y-coordinate\n",
    "        np.sin(3 * t),  # z-coordinate\n",
    "    ]\n",
    ")\n",
    "test_system_fourier = np.zeros((3, 4096))\n",
    "for i in range(3):  # For each dimension\n",
    "    for j in range(10):  # For each mode\n",
    "        freq = np.random.rand() * 2 * np.pi  # Random frequency\n",
    "        phase = np.random.rand() * 2 * np.pi  # Random phase\n",
    "        test_system_fourier[i] += np.sin(freq * t + phase)\n",
    "\n",
    "test_system_noise = np.random.randn(3, 4097).cumsum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fft import rfft\n",
    "\n",
    "from dystformer.attractor import check_power_spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_power_spectrum(dyst_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_power_spectrum(traj: np.ndarray):\n",
    "    power = np.abs(rfft(traj, axis=1)) ** 2  # type: ignore\n",
    "    print(power.shape)\n",
    "    d, n_freqs = power.shape\n",
    "    _, axes = plt.subplots(d, 1, figsize=(10, 2 * d), sharex=True)\n",
    "    x = np.arange(n_freqs)\n",
    "\n",
    "    for i in range(d):\n",
    "        axes[i].plot(x, power[i], \"b-\", color=\"tab:blue\")\n",
    "        axes[i].set_yscale(\"log\")\n",
    "        axes[i].set_ylabel(f\"Dim {i + 1}\")\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    # Set common x-axis label\n",
    "    axes[-1].set_xlabel(\"Frequency\")\n",
    "\n",
    "    # Add a title to the figure\n",
    "    plt.suptitle(\"Power Spectrum\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_power_spectrum(dyst_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_power_spectrum(test_system_fourier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grassberger-Procaccia Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dysts.analysis import gp_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyst_coords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = gp_dim(dyst_coords.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "\n",
    "def compute_gp_dimension(points, num_r=50, scaling_range_idx=None):\n",
    "    \"\"\"\n",
    "    Computes and plots the Grassberger-Procaccia correlation integral and estimates the GP dimension.\n",
    "\n",
    "    Parameters:\n",
    "        points (ndarray): Array of points with shape (T, dim).\n",
    "        num_r (int): Number of r values to consider in the logarithmically spaced range.\n",
    "        scaling_range_idx (tuple or slice): Indices to select the scaling region for the linear fit.\n",
    "                                           If None, a default middle 50% region is used.\n",
    "\n",
    "    Returns:\n",
    "        D2 (float): Estimated correlation (GP) dimension.\n",
    "    \"\"\"\n",
    "    # Define the range of r values based on the pairwise distances\n",
    "    # We use the min and max distance from the data to set our range.\n",
    "    distances = pdist(points, metric=\"euclidean\")\n",
    "    N_pairs = len(distances)\n",
    "    print(distances.shape)\n",
    "\n",
    "    r_min = np.min(distances)\n",
    "    r_max = np.max(distances)\n",
    "    print(f\"r_min: {r_min}, r_max: {r_max}\")\n",
    "    r_min = max(r_min, 1e-10)\n",
    "    # Generate logarithmically spaced r values\n",
    "    r_vals = np.logspace(np.log10(r_min), np.log10(r_max), num_r)\n",
    "\n",
    "    # Compute the correlation integral C(r)\n",
    "    # Compute all pairwise distances (for N points, there are N*(N-1)/2 distances)\n",
    "    # For each r value, count the number of pairs with distance < r\n",
    "    C = np.array([np.sum(distances < r) / N_pairs for r in r_vals])\n",
    "\n",
    "    # Compute logarithms\n",
    "    log_r = np.log10(r_vals)\n",
    "    log_C = np.log10(C)\n",
    "\n",
    "    # Select a scaling region for the linear fit\n",
    "    # Here, if not provided, we choose the middle 50% of the r-range.\n",
    "    if scaling_range_idx is None:\n",
    "        scaling_range = slice(num_r // 4, 3 * num_r // 4)\n",
    "    else:\n",
    "        scaling_range = scaling_range_idx\n",
    "\n",
    "    # Fit a line (using least squares) over the selected scaling region\n",
    "    try:\n",
    "        slope, intercept = np.polyfit(log_r[scaling_range], log_C[scaling_range], 1)\n",
    "        D2 = slope  # The slope corresponds to the correlation dimension D2\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting line: {e}\")\n",
    "        D2 = None\n",
    "        slope = None\n",
    "        intercept = None\n",
    "\n",
    "    print(\"Estimated correlation (GP) dimension D2: {:.3f}\".format(D2))\n",
    "\n",
    "    # Plot the fitted line over the scaling region\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(\n",
    "        log_r,\n",
    "        log_C,\n",
    "        \"o-\",\n",
    "        markersize=3,\n",
    "        label=r\"$\\log C(r)$ vs $\\log r$\",\n",
    "        color=\"tab:blue\",\n",
    "    )\n",
    "    if D2 is not None:\n",
    "        plt.plot(\n",
    "            log_r[scaling_range],\n",
    "            slope * log_r[scaling_range] + intercept,\n",
    "            \"r\",\n",
    "            linewidth=2,\n",
    "            label=rf\"Fit: slope $D_2 \\approx {slope:.3f}$\",\n",
    "            color=\"tab:red\",\n",
    "        )\n",
    "    plt.xlabel(r\"$\\log(r)$\", fontsize=14)\n",
    "    plt.ylabel(r\"$\\log C(r)$\", fontsize=14)\n",
    "    plt.title(\"Correlation Integral (Grassberger-Procaccia)\", fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the GP dimension and plot the results.\n",
    "compute_gp_dimension(dyst_coords.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_gp_dimension(test_system_periodic.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_gp_dimension(test_system_noise.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_gp_dimension(test_system_fourier.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Delay with MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information(x: np.ndarray, y: np.ndarray, bins: int = 64) -> float:\n",
    "    \"\"\"\n",
    "    Compute the mutual information between two 1D arrays x and y\n",
    "        - Uses a 2D histogram with the given number of bins.\n",
    "        - sum_{i,j} p_xy[i,j] * log(p_xy[i,j] / (p_x[i]*p_y[j]))\n",
    "    \"\"\"\n",
    "    pxy, _, _ = np.histogram2d(x, y, bins=bins, density=True)\n",
    "    px = np.sum(pxy, axis=1)\n",
    "    py = np.sum(pxy, axis=0)\n",
    "\n",
    "    mi = 0.0\n",
    "    for i in range(pxy.shape[0]):\n",
    "        for j in range(pxy.shape[1]):\n",
    "            if pxy[i, j] > 0:\n",
    "                mi += pxy[i, j] * (np.log(pxy[i, j]) - np.log(px[i]) - np.log(py[j]))\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_delay(\n",
    "    x: np.ndarray,\n",
    "    max_delay: int = 50,\n",
    "    bins: int = 64,\n",
    "    conv_window_size: int = 3,\n",
    "    first_k_minima_to_consider: int = 3,\n",
    "    plot: bool = False,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Computes the mutual information I(tau) = I( x(t), x(t+tau) ) for tau in {1, 2, ..., max_delay}\n",
    "    Returns the time lag tau corresponding to the first prominent local minimum.\n",
    "\n",
    "    Parameters:\n",
    "        x: 1D array of shape (T,)\n",
    "        max_delay: maximum time lag to consider\n",
    "        bins: number of bins for the histogram\n",
    "        conv_window_size: size of the convolution window for smoothing the MI curve\n",
    "        first_k_minima_to_consider: number of minima to consider for determining first prominent minimum\n",
    "    \"\"\"\n",
    "    mi_values = []\n",
    "    assert x.ndim == 1, \"x must be a 1D array\"\n",
    "    T = len(x)\n",
    "    for tau in range(1, max_delay + 1):\n",
    "        # Use only overlapping segments\n",
    "        mi_tau = mutual_information(x[: T - tau], x[tau:], bins=bins)\n",
    "        mi_values.append(mi_tau)\n",
    "    mi_values = np.array(mi_values)\n",
    "\n",
    "    # Find a prominent local minimum\n",
    "    # 1. smooth the MI curve to reduce noise\n",
    "    smoothed_mi = np.convolve(\n",
    "        mi_values, np.ones(conv_window_size) / conv_window_size, mode=\"valid\"\n",
    "    )\n",
    "\n",
    "    # 2. Calculate the prominence of each minimum\n",
    "    minima_indices = []\n",
    "    prominences = []\n",
    "\n",
    "    # 3. Find all local minima in the smoothed curve\n",
    "    for i in range(1, len(smoothed_mi) - 1):\n",
    "        if smoothed_mi[i] < smoothed_mi[i - 1] and smoothed_mi[i] < smoothed_mi[i + 1]:\n",
    "            minima_indices.append(i)\n",
    "\n",
    "            # Calculate prominence (height difference to nearby values on smoothed MI curve)\n",
    "            left_max = np.max(smoothed_mi[: i + 1])\n",
    "            right_max = np.max(smoothed_mi[i:])\n",
    "            lower_max = min(left_max, right_max)\n",
    "            prominence = lower_max - smoothed_mi[i]\n",
    "            prominences.append(prominence)\n",
    "            if len(prominences) >= first_k_minima_to_consider:\n",
    "                break\n",
    "\n",
    "    # print(prominences)\n",
    "\n",
    "    # If no minima found, return the global minimum\n",
    "    if len(minima_indices) == 0:\n",
    "        first_min = np.argmin(mi_values) + 1\n",
    "    else:\n",
    "        # Find the most prominent minimum among the first first_k_minima_to_consider\n",
    "        num_to_consider = min(first_k_minima_to_consider, len(minima_indices))\n",
    "        best_idx = np.argmax(prominences[:num_to_consider])\n",
    "        # Adjust index to account for smoothing window and 1-based tau\n",
    "        first_min = minima_indices[best_idx] + (conv_window_size // 2) + 1\n",
    "\n",
    "    first_min = int(first_min)\n",
    "\n",
    "    if plot:\n",
    "        taus = np.arange(1, max_delay + 1)\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.plot(taus, mi_values, marker=\"o\", alpha=0.6, label=\"Original MI\")\n",
    "\n",
    "        # Plot smoothed curve\n",
    "        smoothed_taus = (\n",
    "            taus[(conv_window_size // 2) : -(conv_window_size // 2)]\n",
    "            if conv_window_size > 1\n",
    "            else taus\n",
    "        )\n",
    "        if len(smoothed_taus) == len(smoothed_mi):\n",
    "            plt.plot(smoothed_taus, smoothed_mi, \"r-\", linewidth=2, label=\"Smoothed MI\")\n",
    "\n",
    "        # Mark all detected minima\n",
    "        for i, idx in enumerate(minima_indices):\n",
    "            adjusted_idx = idx + (conv_window_size // 2) + 1\n",
    "            if i < num_to_consider:\n",
    "                plt.plot(\n",
    "                    adjusted_idx,\n",
    "                    mi_values[adjusted_idx - 1],\n",
    "                    \"go\",\n",
    "                    markersize=8,\n",
    "                    # label=f\"Minimum τ={adjusted_idx}\" if i == 0 else None,\n",
    "                )\n",
    "            else:\n",
    "                plt.plot(adjusted_idx, mi_values[adjusted_idx - 1], \"yo\", markersize=6)\n",
    "\n",
    "        plt.axvline(\n",
    "            first_min,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            label=f\"Selected min at τ={first_min}\",\n",
    "        )\n",
    "        plt.xlabel(\"Delay τ\")\n",
    "        plt.ylabel(\"Mutual Information\")\n",
    "        plt.title(\"Mutual Information vs. Delay\")\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    return first_min\n",
    "\n",
    "\n",
    "def optimal_sampling_interval(\n",
    "    trajectory, max_delay=100, bins=64, observable=\"x\", k=1, plot=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a trajectory of shape (dim, T), selects an observable and computes the optimal\n",
    "    sampling interval (delay) using the first minimum of mutual information.\n",
    "\n",
    "    Parameters:\n",
    "      trajectory: NumPy array of shape (dim, T)\n",
    "      max_delay: maximum delay (in timesteps) to consider\n",
    "      bins: number of bins for histogram estimation\n",
    "      observable: which observable to use; options: 'x', 'y', 'z', or 'norm'\n",
    "      k: number of minima to consider for determining first prominent minimum\n",
    "      plot: if True, plot mutual information vs. delay\n",
    "\n",
    "    Returns:\n",
    "      tau_opt: the optimal delay (number of timesteps) as determined by the first minimum.\n",
    "    \"\"\"\n",
    "    if observable == \"x\":\n",
    "        x = trajectory[0, :]\n",
    "    elif observable == \"y\":\n",
    "        if trajectory.shape[0] < 2:\n",
    "            raise ValueError(\"Trajectory does not have a second coordinate for 'y'.\")\n",
    "        x = trajectory[1, :]\n",
    "    elif observable == \"z\":\n",
    "        if trajectory.shape[0] < 3:\n",
    "            raise ValueError(\"Trajectory does not have a third coordinate for 'z'.\")\n",
    "        x = trajectory[2, :]\n",
    "    elif observable == \"norm\":\n",
    "        x = np.linalg.norm(trajectory, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown observable. Choose 'x', 'y', 'z', or 'norm'.\")\n",
    "\n",
    "    tau_opt = optimal_delay(\n",
    "        x, max_delay=max_delay, bins=bins, first_k_minima_to_consider=k, plot=plot\n",
    "    )\n",
    "    return tau_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal delay using the first coordinate\n",
    "tau_opt = optimal_sampling_interval(\n",
    "    dyst_coords, max_delay=100, bins=64, observable=\"x\", k=1, plot=True\n",
    ")\n",
    "print(\"Optimal sampling interval (delay τ):\", tau_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-One Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_translation_variables(phi, c):\n",
    "    \"\"\"\n",
    "    Given a scalar observable phi and constant c,\n",
    "    compute the translation variables p(n) and q(n)\n",
    "    as cumulative sums.\n",
    "    \"\"\"\n",
    "    T = len(phi)\n",
    "    n = np.arange(1, T + 1)\n",
    "    # Compute p(n) and q(n)\n",
    "    p = np.cumsum(phi * np.cos(c * n))\n",
    "    q = np.cumsum(phi * np.sin(c * n))\n",
    "    return p, q\n",
    "\n",
    "\n",
    "def compute_mean_square_displacement(p, q, max_shift_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Computes the mean square displacement (MSD) over a range of shifts.\n",
    "\n",
    "    Parameters:\n",
    "      p, q: translation variables (1D arrays)\n",
    "      max_shift_ratio: maximum fraction of the length to use for shifts.\n",
    "\n",
    "    Returns:\n",
    "      n_vals: array of shift indices\n",
    "      MSD: array of mean square displacements corresponding to n_vals.\n",
    "    \"\"\"\n",
    "    T = len(p)\n",
    "    max_shift = int(max_shift_ratio * T)\n",
    "    n_vals = np.arange(1, max_shift + 1)\n",
    "    MSD = np.empty_like(n_vals, dtype=float)\n",
    "\n",
    "    # For each time shift n, compute the mean squared difference\n",
    "    for idx, n in enumerate(n_vals):\n",
    "        diff_p = p[n:] - p[:-n]\n",
    "        diff_q = q[n:] - q[:-n]\n",
    "        MSD[idx] = np.mean(diff_p**2 + diff_q**2)\n",
    "    return n_vals, MSD\n",
    "\n",
    "\n",
    "def compute_K_statistic(n_vals, MSD):\n",
    "    \"\"\"\n",
    "    Computes the correlation coefficient between the shift indices and the MSD.\n",
    "    A value near 1 indicates linear growth (chaos), while near 0 indicates bounded behavior.\n",
    "    \"\"\"\n",
    "    # np.corrcoef returns a 2x2 correlation matrix.\n",
    "    corr_matrix = np.corrcoef(n_vals, MSD)\n",
    "    K = corr_matrix[0, 1]\n",
    "    return K\n",
    "\n",
    "\n",
    "def zero_one_test(phi, c=None, threshold=0.5, plot=False):\n",
    "    \"\"\"\n",
    "    Performs the 0–1 test for chaos on a scalar observable.\n",
    "\n",
    "    Parameters:\n",
    "      phi: 1D NumPy array representing the observable (length T).\n",
    "      c: constant in (0,pi); if None, a random value in (pi/5, 4*pi/5) is chosen to avoid resonances.\n",
    "      threshold: threshold on |K| to decide if the system is chaotic.\n",
    "      plot: if True, plots the (p, q) trajectory and MSD vs. shift index.\n",
    "\n",
    "    Returns:\n",
    "      K: the computed correlation coefficient.\n",
    "      is_chaotic: boolean, True if |K| > threshold.\n",
    "    \"\"\"\n",
    "    if c is None:\n",
    "        # Choosing c in (pi/5, 4*pi/5) can help avoid resonances.\n",
    "        c = np.random.uniform(np.pi / 10, 1 * np.pi / 5)\n",
    "\n",
    "    p, q = compute_translation_variables(phi, c)\n",
    "    n_vals, MSD = compute_mean_square_displacement(p, q)\n",
    "    K = compute_K_statistic(n_vals, MSD)\n",
    "\n",
    "    if plot:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        axs[0].plot(p, q, lw=1)\n",
    "        axs[0].set_title(\"Translation Variables (p, q)\")\n",
    "        axs[0].set_xlabel(\"p\")\n",
    "        axs[0].set_ylabel(\"q\")\n",
    "        axs[1].plot(n_vals, MSD, \"o-\", lw=1)\n",
    "        axs[1].set_title(\"Mean Square Displacement (MSD)\")\n",
    "        axs[1].set_xlabel(\"n (shift)\")\n",
    "        axs[1].set_ylabel(\"MSD\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    is_chaotic = np.abs(K) > threshold\n",
    "    return K, is_chaotic\n",
    "\n",
    "\n",
    "def test_0_1_for_chaos(traj, observable=\"x\", c=None, threshold=0.5, plot=False):\n",
    "    \"\"\"\n",
    "    Applies the 0–1 test for chaos to a multidimensional trajectory.\n",
    "\n",
    "    Parameters:\n",
    "      traj: NumPy array of shape (dim, T).\n",
    "      observable: which observable to use; options:\n",
    "                  'x', 'y', 'z' (for individual coordinates) or 'norm' (Euclidean norm of the state vector).\n",
    "      threshold: threshold on the K statistic to decide if chaotic.\n",
    "      plot: if True, produces diagnostic plots.\n",
    "\n",
    "    Returns:\n",
    "      K: computed K-statistic.\n",
    "      is_chaotic: boolean, True if the test indicates chaos.\n",
    "    \"\"\"\n",
    "    dim, T = traj.shape\n",
    "\n",
    "    if observable == \"norm\":\n",
    "        # Use the Euclidean norm of the state as the observable.\n",
    "        phi = np.linalg.norm(traj, axis=0)\n",
    "    elif observable == \"x\":\n",
    "        phi = traj[0, :]\n",
    "    elif observable == \"y\":\n",
    "        if dim < 2:\n",
    "            raise ValueError(\"Trajectory does not have a y dimension.\")\n",
    "        phi = traj[1, :]\n",
    "    elif observable == \"z\":\n",
    "        if dim < 3:\n",
    "            raise ValueError(\"Trajectory does not have a z dimension.\")\n",
    "        phi = traj[2, :]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid observable. Choose 'x', 'y', 'z', or 'norm'.\")\n",
    "\n",
    "    K, is_chaotic = zero_one_test(phi, c=c, threshold=threshold, plot=plot)\n",
    "    return K, is_chaotic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test using the Euclidean norm as observable.\n",
    "# K, chaotic = test_0_1_for_chaos(\n",
    "#     dyst_coords[:, ::tau_opt], observable=\"x\", c=default_c_val, threshold=0.5, plot=True\n",
    "# )\n",
    "# print(\"K-statistic:\", K)\n",
    "# print(\"Chaotic:\", chaotic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test using the Euclidean norm as observable.\n",
    "def run_zero_one_sweep(\n",
    "    traj,\n",
    "    c_vals: np.ndarray,\n",
    "    observable=\"x\",\n",
    "    subsample_interval: int = 1,\n",
    "    threshold: float = 0.5,\n",
    ") -> tuple[float, np.ndarray]:\n",
    "    K_vals = []\n",
    "    if subsample_interval > 1:\n",
    "        traj = traj[:, ::subsample_interval]\n",
    "    for c_val in c_vals:\n",
    "        K, _ = test_0_1_for_chaos(\n",
    "            traj,\n",
    "            observable=observable,\n",
    "            c=c_val,\n",
    "            threshold=threshold,\n",
    "            plot=False,\n",
    "        )\n",
    "        K_vals.append(K)\n",
    "\n",
    "    K_vals = np.array(K_vals)\n",
    "    chaos_score = float(np.sum(K_vals > threshold) / len(K_vals))\n",
    "    return chaos_score, K_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vals = np.random.uniform(np.pi / 5, 4 * np.pi / 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transient_prop = 0.5\n",
    "len_transient = int(transient_prop * dyst_coords.shape[1])\n",
    "print(f\"Transient length: {len_transient}\")\n",
    "chaos_score, K_vals = run_zero_one_sweep(\n",
    "    dyst_coords[:, len_transient:],\n",
    "    c_vals=c_vals,\n",
    "    observable=\"z\",\n",
    "    subsample_interval=tau_opt,\n",
    "    threshold=0.25,\n",
    ")\n",
    "print(f\"Dyst Test chaos score: {chaos_score}\")\n",
    "print(\"median K\", np.median(K_vals))\n",
    "print(\"mean K\", np.mean(K_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(c_vals, K_vals)\n",
    "plt.xlabel(\"c\")\n",
    "plt.ylabel(\"K\")\n",
    "plt.title(\"K-statistic vs c\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test using the Euclidean norm as observable.\n",
    "# K, chaotic = test_0_1_for_chaos(\n",
    "#     test_system_fourier[:, ::tau_opt],\n",
    "#     observable=\"x\",\n",
    "#     c=default_c_val,\n",
    "#     threshold=0.5,\n",
    "#     plot=True,\n",
    "# )\n",
    "# print(\"K-statistic:\", K)\n",
    "# print(\"Chaotic:\", chaotic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaos_score, K_vals = run_zero_one_sweep(\n",
    "#     test_system_fourier,\n",
    "#     c_vals=c_vals,\n",
    "#     observable=\"x\",\n",
    "#     subsample_interval=tau_opt,\n",
    "#     threshold=0.5,\n",
    "# )\n",
    "# print(f\"Fourier Test chaos score: {chaos_score}\")\n",
    "# print(\"median K\", np.median(K_vals))\n",
    "# print(\"mean K\", np.mean(K_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test using the Euclidean norm as observable.\n",
    "# K, chaotic = test_0_1_for_chaos(\n",
    "#     test_system_periodic[:, ::tau_opt],\n",
    "#     observable=\"x\",\n",
    "#     c=default_c_val,\n",
    "#     threshold=0.5,\n",
    "#     plot=True,\n",
    "# )\n",
    "# print(\"K-statistic:\", K)\n",
    "# print(\"Chaotic:\", chaotic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaos_score, K_vals = run_zero_one_sweep(\n",
    "#     test_system_periodic,\n",
    "#     c_vals=c_vals,\n",
    "#     observable=\"x\",\n",
    "#     subsample_interval=tau_opt,\n",
    "#     threshold=0.5,\n",
    "# )\n",
    "# print(f\"Periodic Test chaos score: {chaos_score}\")\n",
    "# print(\"median K\", np.median(K_vals))\n",
    "# print(\"mean K\", np.mean(K_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test using the Euclidean norm as observable.\n",
    "# K, chaotic = test_0_1_for_chaos(\n",
    "#     test_system_noise[:, ::tau_opt],\n",
    "#     observable=\"x\",\n",
    "#     c=default_c_val,\n",
    "#     threshold=0.5,\n",
    "#     plot=True,\n",
    "# )\n",
    "# print(\"K-statistic:\", K)\n",
    "# print(\"Chaotic:\", chaotic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chaos_score, K_vals = run_zero_one_sweep(\n",
    "#     test_system_noise,\n",
    "#     c_vals=c_vals,\n",
    "#     observable=\"x\",\n",
    "#     subsample_interval=tau_opt,\n",
    "#     threshold=0.5,\n",
    "# )\n",
    "# print(f\"Noise Test chaos score: {chaos_score}\")\n",
    "# print(\"median K\", np.median(K_vals))\n",
    "# print(\"mean K\", np.mean(K_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z1test(x, c_min=np.pi / 5, c_max=4 * np.pi / 5):\n",
    "    \"\"\"\n",
    "    Gottwald-Melbourne 0-1 test for chaos.\n",
    "\n",
    "    Parameters:\n",
    "        x (array-like): Input time series data.\n",
    "\n",
    "    Returns:\n",
    "        kmedian (float): Test statistic near 0 for non-chaotic data and near 1 for chaotic data.\n",
    "\n",
    "    Notes:\n",
    "        - The function generates 100 random c-values in the interval [pi/5, 4pi/5].\n",
    "        - It computes cumulative sums p and q and then calculates a mean-square displacement M.\n",
    "        - Finally, the Pearson correlation between t and M is computed for each c-value,\n",
    "          and the median of these correlations is returned.\n",
    "\n",
    "    Translated from MATLAB code provided by Paul Matthews https://www.mathworks.com/matlabcentral/fileexchange/25050-0-1-test-for-chaos\n",
    "        based on the method proposed by Gottwald and Melbourne\n",
    "    \"\"\"\n",
    "    # Ensure x is a 1D numpy array\n",
    "    x = np.asarray(x).flatten()\n",
    "    N = len(x)\n",
    "    j = np.arange(1, N + 1)  # equivalent to MATLAB's [1:N]\n",
    "\n",
    "    # t runs from 1 to round(N/10)\n",
    "    t_max = int(round(N / 10))\n",
    "    t = np.arange(1, t_max + 1)\n",
    "    M = np.zeros(t_max)\n",
    "\n",
    "    # 100 random c values in [pi/5, 4pi/5]\n",
    "    c = c_min + np.random.rand(100) * (c_max - c_min)\n",
    "    kcorr = np.zeros(100)\n",
    "\n",
    "    # Loop over each random c value\n",
    "    for its in range(100):\n",
    "        c_val = c[its]\n",
    "        # Compute cumulative sums p and q\n",
    "        p = np.cumsum(x * np.cos(j * c_val))\n",
    "        q = np.cumsum(x * np.sin(j * c_val))\n",
    "\n",
    "        # Compute M(n) for n = 1, ..., round(N/10)\n",
    "        for n in range(1, t_max + 1):\n",
    "            # p[n:] corresponds to MATLAB p(n+1:N) and p[:-n] to p(1:N-n)\n",
    "            diff_p = p[n:] - p[:-n]\n",
    "            diff_q = q[n:] - q[:-n]\n",
    "            term = np.mean(diff_p**2 + diff_q**2)\n",
    "            correction = (\n",
    "                (np.mean(x) ** 2) * (1 - np.cos(n * c_val)) / (1 - np.cos(c_val))\n",
    "            )\n",
    "            M[n - 1] = term - correction\n",
    "\n",
    "        # Compute the Pearson correlation coefficient between t and M\n",
    "        corr_matrix = np.corrcoef(t, M)\n",
    "        kcorr[its] = corr_matrix[0, 1]\n",
    "\n",
    "    # Two crude attempts to check for oversampling:\n",
    "    condition1 = (np.max(x) - np.min(x)) / np.mean(np.abs(np.diff(x))) > 10\n",
    "    median_lower = np.median(kcorr[c < np.mean(c)])\n",
    "    median_upper = np.median(kcorr[c > np.mean(c)])\n",
    "    condition2 = (median_lower - median_upper) > 0.5\n",
    "    if condition1 or condition2:\n",
    "        print(\"Warning: data is probably oversampled.\")\n",
    "        print(\"Use coarser sampling or reduce the maximum value of c.\")\n",
    "\n",
    "    return np.median(kcorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z1test(dyst_coords[0, ::tau_opt], c_min=np.pi / 5, c_max=4 * np.pi / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z1test(test_system_fourier[0, ::tau_opt], c_min=np.pi / 5, c_max=4 * np.pi / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z1test(test_system_periodic[0, ::tau_opt], c_min=np.pi / 5, c_max=4 * np.pi / 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z1test(test_system_noise[0, ::tau_opt], c_min=np.pi / 5, c_max=4 * np.pi / 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit Cycle Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_limit_cycle(\n",
    "    traj: np.ndarray,\n",
    "    tolerance: float = 1e-3,\n",
    "    min_prop_recurrences: float = 0.0,\n",
    "    min_counts_per_rtime: int = 100,\n",
    "    min_block_length: int = 1,\n",
    "    min_recurrence_time: int = 1,\n",
    "    enforce_endpoint_recurrence: bool = False,\n",
    "    return_computed_quantities: bool = False,\n",
    ") -> bool | tuple[bool, dict]:\n",
    "    \"\"\"\n",
    "    limit cycle test from attractor.py, exposed here for plotting purposes\n",
    "    Returns: True if the trajectory is not a limit cycle, False otherwise\n",
    "        If False and also return_computed_quantities is True, returns a tuple (False, computed_quantities)\n",
    "    \"\"\"\n",
    "    n = traj.shape[1]\n",
    "\n",
    "    # Step 1: Calculate the pairwise distance matrix, shape should be (N, N)\n",
    "    dist_matrix = cdist(traj.T, traj.T, metric=\"euclidean\").astype(np.float16)\n",
    "    dist_matrix = np.triu(dist_matrix, k=1)\n",
    "\n",
    "    # Step 2: Get recurrence times from thresholding distance matrix\n",
    "    recurrence_indices = np.asarray(\n",
    "        (dist_matrix < tolerance) & (dist_matrix > 0)\n",
    "    ).nonzero()\n",
    "\n",
    "    n_recurrences = len(recurrence_indices[0])\n",
    "    if n_recurrences == 0:\n",
    "        return True\n",
    "\n",
    "    if enforce_endpoint_recurrence:\n",
    "        # check if an eps neighborhood around either n-1 or 0 is in either of the recurrence indices\n",
    "        eps = 0\n",
    "        if not any(\n",
    "            (n - 1) - max(indices) <= eps or min(indices) - 0 <= eps\n",
    "            for indices in recurrence_indices\n",
    "        ):\n",
    "            return True\n",
    "\n",
    "    # get recurrence times\n",
    "    recurrence_times = np.abs(recurrence_indices[0] - recurrence_indices[1])\n",
    "    recurrence_times = recurrence_times[recurrence_times >= min_recurrence_time]\n",
    "\n",
    "    # Heuristic 1: Check if there are enough recurrences to consider a limit cycle\n",
    "    n_recurrences = len(recurrence_times)\n",
    "    if n_recurrences < int(min_prop_recurrences * n):\n",
    "        return True\n",
    "\n",
    "    # Heuristic 2: Check if there are enough valid recurrence times\n",
    "    rtimes_counts = Counter(recurrence_times)\n",
    "    n_valid_rtimes = sum(\n",
    "        1 for count in rtimes_counts.values() if count >= min_counts_per_rtime\n",
    "    )\n",
    "    if n_valid_rtimes < 1:\n",
    "        return True\n",
    "\n",
    "    # Heuristic 3: Check if the valid recurrence times are formed of blocks of consecutive timepoints\n",
    "    if min_block_length > 1:\n",
    "        rtimes_dict = defaultdict(list)\n",
    "        block_length = 1\n",
    "        prev_rtime = None\n",
    "        prev_t1 = None\n",
    "        prev_t2 = None\n",
    "        rtimes_is_valid = False\n",
    "        num_blocks = 0\n",
    "        # assuming recurrence_indices[0] is sorted\n",
    "        for t1, t2 in zip(*recurrence_indices):\n",
    "            rtime = abs(t2 - t1)\n",
    "            if rtime < min_recurrence_time:\n",
    "                continue\n",
    "            if (\n",
    "                rtime == prev_rtime\n",
    "                and abs(t1 - prev_t1) == 1\n",
    "                and abs(t2 - prev_t2) == 1\n",
    "            ):\n",
    "                block_length += 1\n",
    "            else:\n",
    "                if block_length > min_block_length:\n",
    "                    rtimes_dict[prev_rtime].append(block_length)\n",
    "                    num_blocks += 1\n",
    "                block_length = 1\n",
    "            prev_t1, prev_t2, prev_rtime = t1, t2, rtime\n",
    "            if block_length > min_block_length * 2:\n",
    "                rtimes_is_valid = True\n",
    "                break\n",
    "            if num_blocks >= 2:  # if valid, save computation and break\n",
    "                rtimes_is_valid = True\n",
    "                break\n",
    "        if not rtimes_is_valid:\n",
    "            return True\n",
    "\n",
    "    computed_quantities = {\n",
    "        \"dist_matrix\": dist_matrix,\n",
    "        \"recurrence_indices\": recurrence_indices,\n",
    "        \"recurrence_times\": recurrence_times,\n",
    "    }\n",
    "    if return_computed_quantities:\n",
    "        return False, computed_quantities\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_not_limit_cycle_result = check_not_limit_cycle(\n",
    "#     dyst_coords,\n",
    "#     tolerance=1e-3,\n",
    "#     min_prop_recurrences=0.1,\n",
    "#     min_counts_per_rtime=200,\n",
    "#     min_block_length=50,\n",
    "#     enforce_endpoint_recurrence=True,\n",
    "#     return_computed_quantities=True,\n",
    "# )\n",
    "# print(is_not_limit_cycle_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recurrence_times(\n",
    "    traj: np.ndarray,\n",
    "    dist_matrix: np.ndarray,\n",
    "    recurrence_times: np.ndarray,\n",
    "    recurrence_indices: np.ndarray,\n",
    "):\n",
    "    dyst_name = system_name.split(\"_\")[0]\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "    ax1.hist(recurrence_times, bins=100, edgecolor=\"black\")\n",
    "    ax1.set_xlabel(\"Recurrence Time\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.set_title(\"Recurrence Times\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    xyz = traj[:3, :]\n",
    "    xyz1 = xyz[:, : int(n / 2)]\n",
    "    xyz2 = xyz[:, int(n / 2) :]\n",
    "    ic_point = traj[:3, 0]\n",
    "    final_point = traj[:3, -1]\n",
    "    ax2 = fig.add_subplot(312, projection=\"3d\")\n",
    "    ax2.plot(*xyz1, alpha=0.5, linewidth=1, color=\"tab:blue\")\n",
    "    ax2.plot(*xyz2, alpha=0.5, linewidth=1, color=\"tab:orange\")\n",
    "    ax2.scatter(*ic_point, marker=\"*\", s=100, alpha=0.5, color=\"tab:blue\")\n",
    "    ax2.scatter(*final_point, marker=\"x\", s=100, alpha=0.5, color=\"tab:orange\")\n",
    "    ax2.set_xlabel(\"X\")\n",
    "    ax2.set_ylabel(\"Y\")\n",
    "    ax2.set_zlabel(\"Z\")  # type: ignore\n",
    "    ax2.set_title(dyst_name)\n",
    "\n",
    "    ax3 = fig.add_subplot(313)\n",
    "    X, Y = np.meshgrid(np.arange(dist_matrix.shape[0]), np.arange(dist_matrix.shape[1]))\n",
    "    pcolormesh = ax3.pcolormesh(\n",
    "        X,\n",
    "        Y,\n",
    "        dist_matrix,\n",
    "        cmap=\"viridis_r\",\n",
    "        shading=\"auto\",\n",
    "        norm=colors.LogNorm(),\n",
    "    )\n",
    "    plt.colorbar(pcolormesh, ax=ax3)\n",
    "    ax3.scatter(\n",
    "        recurrence_indices[0],\n",
    "        recurrence_indices[1],\n",
    "        color=\"black\",\n",
    "        s=20,\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax3.set_title(\"Recurrence Distance Matrix\")\n",
    "    ax3.set_xlabel(\"Time\")\n",
    "    ax3.set_ylabel(\"Time\")\n",
    "    ax3.set_aspect(\"equal\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
