{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../custom_style.mplstyle\"):\n",
    "    plt.style.use([\"ggplot\", \"../custom_style.mplstyle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../figs\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"final_skew40/test_zeroshot\"\n",
    "\n",
    "run_metrics_dir_dict = {\n",
    "    \"Our Model\": os.path.join(\n",
    "        \"../eval_results\",\n",
    "        \"patchtst\",\n",
    "        \"pft_stand_rff_only_pretrained-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M Finetune\": os.path.join(\n",
    "        \"../eval_results\",\n",
    "        \"chronos\",\n",
    "        \"chronos_finetune_stand_updated-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M\": os.path.join(\n",
    "        \"../eval_results\",\n",
    "        \"chronos\",\n",
    "        \"chronos_mini_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Time MOE 50M\": os.path.join(\n",
    "        \"../eval_results\",\n",
    "        \"timemoe-50m\",\n",
    "        data_split,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dir_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs = defaultdict(dict)\n",
    "for model_name, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "    print(model_name)\n",
    "    for file in sorted(\n",
    "        os.listdir(run_metrics_dir),\n",
    "        key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "    ):\n",
    "        if file.endswith(\".csv\"):\n",
    "            prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "            # print(prediction_length)\n",
    "            with open(os.path.join(run_metrics_dir, file), \"r\") as f:\n",
    "                metrics = pd.read_csv(f).to_dict()\n",
    "                metrics_all_runs[model_name][prediction_length] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs[\"Our Model\"][64].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics = defaultdict(dict)\n",
    "for model_name, all_metrics_of_model in metrics_all_runs.items():\n",
    "    print(model_name)\n",
    "    for prediction_length, metrics in all_metrics_of_model.items():\n",
    "        # print(f\"prediction_length: {prediction_length}\")\n",
    "        systems = metrics[\"system\"]\n",
    "        metrics_unrolled = {\n",
    "            k: list(v.values()) for k, v in metrics.items() if k != \"system\"\n",
    "        }\n",
    "        print(metrics_unrolled.keys())\n",
    "        unrolled_metrics[model_name][prediction_length] = metrics_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(unrolled_metrics[\"Our Model\"][128][\"r2_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Our Model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Our Model\"][64].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unrolled_metrics[\"Our Model\"][64][\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_strip_plot(\n",
    "#     unrolled_metrics: dict[str, dict[int, dict[str, list[float]]]],\n",
    "#     prediction_length: int,\n",
    "#     run_names: list[str],\n",
    "#     ylim=(1e-5, 1e5),\n",
    "#     verbose: bool = False,\n",
    "# ):\n",
    "#     metrics_by_run_name = {\n",
    "#         run_name: unrolled_metrics[run_name][prediction_length]\n",
    "#         for run_name in run_names\n",
    "#     }\n",
    "#     metric_names = list(metrics_by_run_name[run_names[0]].keys())\n",
    "#     metrics_to_exclude = [\"spearman\"]\n",
    "#     metric_names = [name for name in metric_names if name not in metrics_to_exclude]\n",
    "#     metric_names_title = []\n",
    "#     for metric_name in metric_names:\n",
    "#         metric_name = metric_name.replace(\"_\", \" \")\n",
    "#         if metric_name in [\"mse\", \"mae\", \"rmse\", \"mape\", \"smape\"]:\n",
    "#             metric_names_title.append(metric_name.upper())\n",
    "#         else:\n",
    "#             metric_names_title.append(metric_name.capitalize())\n",
    "\n",
    "#     colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "#     plt.figure(figsize=(5, 3))\n",
    "#     for i, metric_name in enumerate(metric_names):\n",
    "#         metric_name_title = metric_names_title[i]\n",
    "#         # Calculate offsets for each run within a metric group\n",
    "#         width = 0.4  # total width for the group\n",
    "#         offset = width / (len(run_names) - 1) if len(run_names) > 1 else 0\n",
    "\n",
    "#         for j, run_name in enumerate(run_names):\n",
    "#             values = metrics_by_run_name[run_name][metric_name]\n",
    "#             # Shift each run's points by their offset\n",
    "#             x_shift = 0.5 * (j - (len(run_names) - 1) / 2)\n",
    "#             x_pos = i + x_shift\n",
    "\n",
    "#             # plt.scatter(\n",
    "#             #     [x_pos] * len(values),\n",
    "#             #     values,\n",
    "#             #     label=run_name if i == 0 else None,\n",
    "#             #     alpha=0.5,\n",
    "#             #     s=16,\n",
    "#             #     color=colors[j],\n",
    "#             # )\n",
    "\n",
    "#             sns.stripplot(\n",
    "#                 x=[metric_name_title] * len(values),\n",
    "#                 y=values,\n",
    "#                 label=run_name if i == 0 else None,\n",
    "#                 alpha=0.5,\n",
    "#                 size=4,\n",
    "#                 color=colors[j],\n",
    "#                 jitter=0.1,\n",
    "#                 # dodge=j * 0.3,\n",
    "#             )\n",
    "\n",
    "#             median_value = np.median(values)\n",
    "#             if verbose:\n",
    "#                 print(f\"{metric_name_title} median: {median_value}\")\n",
    "\n",
    "#             plt.hlines(\n",
    "#                 y=median_value,\n",
    "#                 xmin=x_pos - offset / 2,\n",
    "#                 xmax=x_pos + offset / 2,\n",
    "#                 color=colors[j],\n",
    "#                 linewidth=2,\n",
    "#                 zorder=3,\n",
    "#             )\n",
    "\n",
    "#     # Set x-ticks at metric positions\n",
    "#     plt.xticks(\n",
    "#         range(len(metric_names_title)),\n",
    "#         metric_names_title,\n",
    "#     )\n",
    "\n",
    "#     plt.legend(frameon=True, fontsize=6)\n",
    "#     plt.yscale(\"log\")\n",
    "#     plt.ylim(ylim)\n",
    "#     plt.title(\"Metrics\", fontweight=\"bold\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def make_strip_plot(\n",
    "    unrolled_metrics: dict[str, dict[int, dict[str, list[float]]]],\n",
    "    prediction_length: int,\n",
    "    run_names: list[str],\n",
    "    ylim=(1e-5, 1e5),\n",
    "    verbose: bool = False,\n",
    "    metrics_to_exclude: list[str] = [],\n",
    "    use_rescaled_smape: bool = False,\n",
    "    run_names_to_exclude: list[str] = [],\n",
    "    plot_violin: bool = False,\n",
    "    title: str = \"Metrics\",\n",
    "    save_path: str | None = None,\n",
    "):\n",
    "    # Extract metrics data for the given prediction_length and run_names\n",
    "    metrics_by_run_name = {\n",
    "        run_name: unrolled_metrics[run_name][prediction_length]\n",
    "        for run_name in run_names\n",
    "    }\n",
    "    metric_names = list(metrics_by_run_name[run_names[0]].keys())\n",
    "    metric_names = [name for name in metric_names if name not in metrics_to_exclude]\n",
    "    run_names = [name for name in run_names if name not in run_names_to_exclude]\n",
    "\n",
    "    # # Create pretty titles for x-axis tick labels\n",
    "    # metric_names_title = [\n",
    "    #     name.replace(\"_\", \" \").upper()\n",
    "    #     if name in [\"mse\", \"mae\", \"rmse\", \"mape\", \"smape\"]\n",
    "    #     else name.replace(\"_\", \" \").capitalize()\n",
    "    #     for name in metric_names\n",
    "    # ]\n",
    "\n",
    "    metric_names_title = []\n",
    "    for name in metric_names:\n",
    "        # Create pretty titles for x-axis tick labels\n",
    "        if name in [\"mse\", \"mae\", \"rmse\", \"mape\"]:\n",
    "            name = name.upper()\n",
    "        elif name == \"smape\":\n",
    "            name = \"sMAPE\"\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        metric_names_title.append(name)\n",
    "\n",
    "    colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    plot_data = []\n",
    "    median_data = []\n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        metric_title = metric_names_title[i]\n",
    "        for run_name in run_names:\n",
    "            values = metrics_by_run_name[run_name][metric_name]\n",
    "            if metric_name == \"smape\" and use_rescaled_smape:\n",
    "                values = [x / 100 for x in values]\n",
    "            median_value = np.nanmedian(values)\n",
    "            plot_data.extend([(metric_title, v, run_name) for v in values])\n",
    "            median_data.append((metric_title, median_value, run_name))\n",
    "            if verbose:\n",
    "                print(f\"{metric_title} - {run_name} median: {median_value}\")\n",
    "\n",
    "    # Create DataFrame for use with seaborn\n",
    "    df = pd.DataFrame(plot_data, columns=[\"Metric\", \"Value\", \"Run\"])\n",
    "\n",
    "    if plot_violin:\n",
    "        # Plot violin plot\n",
    "        ax = sns.violinplot(\n",
    "            data=df,\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Run\",\n",
    "            dodge=True,\n",
    "            alpha=0.7,\n",
    "            palette=colors[: len(run_names)],\n",
    "            scale=\"width\",\n",
    "            cut=0,\n",
    "        )\n",
    "    else:\n",
    "        # Plot box plot\n",
    "        ax = sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Run\",\n",
    "            dodge=True,\n",
    "            width=0.8,\n",
    "            fliersize=0,  # Don't show outlier points\n",
    "            palette=colors[: len(run_names)],\n",
    "            saturation=0.5,\n",
    "        )\n",
    "\n",
    "        # Plot the strip plot using hue (with dodge enabled)\n",
    "        ax = sns.stripplot(\n",
    "            data=df,\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Run\",\n",
    "            dodge=True,  # enables side-by-side grouping for each metric\n",
    "            alpha=0.4,\n",
    "            size=2,\n",
    "            jitter=0.15,\n",
    "            palette=colors[: len(run_names)],\n",
    "            order=metric_names_title,\n",
    "            legend=False,\n",
    "            zorder=2,\n",
    "            rasterized=True,\n",
    "        )\n",
    "\n",
    "    # Get the center x-coordinate for each metric category\n",
    "    x_positions = {\n",
    "        label.get_text(): pos for pos, label in enumerate(ax.get_xticklabels())\n",
    "    }\n",
    "\n",
    "    # Assume each category uses a total width of 0.8 (default in many seaborn categorical plots)\n",
    "    dodge_width = 0.8\n",
    "    n = len(run_names)\n",
    "    # The space allocated per hue (run) within the category:\n",
    "    group_width = dodge_width / n\n",
    "    # Let the median line be slightly wider than the group width\n",
    "    median_line_width = group_width * 0.8\n",
    "\n",
    "    # Draw horizontal lines for medians centered exactly on the corresponding dodge positions\n",
    "    for metric, median_value, run in median_data:\n",
    "        x_center = x_positions[metric]\n",
    "        run_index = run_names.index(run)\n",
    "        # Compute the dodge offset so that the hues are evenly spaced\n",
    "        offset = (run_index - (n - 1) / 2) * group_width\n",
    "        x_line_center = x_center + offset\n",
    "        plt.hlines(\n",
    "            y=median_value,\n",
    "            xmin=x_line_center - median_line_width / 2,\n",
    "            xmax=x_line_center + median_line_width / 2,\n",
    "            color=\"black\",\n",
    "            linewidth=2,\n",
    "            zorder=3,\n",
    "        )\n",
    "\n",
    "    # plt.yscale(\"log\")\n",
    "    plt.ylim(ylim)\n",
    "    plt.ylabel(\"\")\n",
    "    plt.legend(frameon=True, fontsize=7)\n",
    "    plt.title(title, fontweight=\"bold\")\n",
    "    plt.xticks(\n",
    "        rotation=15, fontweight=\"bold\"\n",
    "    )  # Optional: rotates x-tick labels for readability\n",
    "    plt.xlabel(\"\")\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_smape_ours = np.max(unrolled_metrics[\"Our Model\"][128][\"smape\"])\n",
    "# max_smape_chronos = np.max(unrolled_metrics[\"Chronos Finetune\"][128][\"smape\"])\n",
    "# max_smape_val = max(max_smape_ours, max_smape_chronos)\n",
    "# print(max_smape_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median_r2score_ours = np.median(unrolled_metrics[\"Our Model\"][128][\"r2_score\"])\n",
    "# median_r2score_chronos = np.median(\n",
    "#     unrolled_metrics[\"Chronos Finetune\"][128][\"r2_score\"]\n",
    "# )\n",
    "# print(median_r2score_ours, median_r2score_chronos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"path.simplify_threshold\"] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_strip_plot(\n",
    "    unrolled_metrics,\n",
    "    128,\n",
    "    run_names=[\n",
    "        \"Our Model\",\n",
    "        \"Chronos 20M Finetune\",\n",
    "        \"Chronos 20M\",\n",
    "        \"Time MOE 50M\",\n",
    "    ],\n",
    "    ylim=(0, 2),\n",
    "    metrics_to_exclude=[\"r2_score\"],\n",
    "    run_names_to_exclude=[\"pft_stand_pretrained_vanilla\"],\n",
    "    use_rescaled_smape=True,\n",
    "    plot_violin=False,\n",
    "    title=\"Metrics for Zeroshot Systems\",\n",
    "    save_path=\"zeroshot_metrics_strip_bar_plot.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_strip_plot(unrolled_metrics, 128, [\"Chronos Finetune\"], ylim=(1e-5, 1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_metrics_dict(\n",
    "    unrolled_metrics: dict, metric_name: str\n",
    ") -> dict[str, dict[str, list[float]]]:\n",
    "    summary_metrics_dict = defaultdict(dict)\n",
    "    for model_name, metrics_dict in unrolled_metrics.items():\n",
    "        print(model_name)\n",
    "        prediction_lengths = list(metrics_dict.keys())\n",
    "        summary_metrics_dict[model_name][\"prediction_lengths\"] = prediction_lengths\n",
    "        means = []\n",
    "        medians = []\n",
    "        stds = []\n",
    "        for prediction_length in prediction_lengths:\n",
    "            metric_val = metrics_dict[prediction_length][metric_name]\n",
    "            means.append(np.nanmean(metric_val))\n",
    "            medians.append(np.nanmedian(metric_val))\n",
    "            stds.append(np.nanstd(metric_val))\n",
    "        summary_metrics_dict[model_name][\"means\"] = means\n",
    "        summary_metrics_dict[model_name][\"medians\"] = medians\n",
    "        summary_metrics_dict[model_name][\"stds\"] = stds\n",
    "    return summary_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict = get_summary_metrics_dict(unrolled_metrics, \"smape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict[\"Ours\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_prediction_length(\n",
    "    metrics_dict, metric_name, show_std_envelope=False\n",
    "):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    for model_name, metrics in metrics_dict.items():\n",
    "        plt.plot(\n",
    "            metrics[\"prediction_lengths\"],\n",
    "            metrics[\"medians\"],\n",
    "            marker=\"o\",\n",
    "            label=model_name,\n",
    "        )\n",
    "        std_envelope = np.array(metrics[\"stds\"])\n",
    "        if show_std_envelope:\n",
    "            plt.fill_between(\n",
    "                metrics[\"prediction_lengths\"],\n",
    "                metrics[\"means\"] - std_envelope,\n",
    "                metrics[\"means\"] + std_envelope,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Prediction Length\")\n",
    "    plt.title(metric_name, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics_by_prediction_length(smape_metrics_dict, \"sMAPE\", show_std_envelope=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict = {\n",
    "    metrics_name: get_summary_metrics_dict(unrolled_metrics, metrics_name)\n",
    "    for metrics_name in [\n",
    "        \"mse\",\n",
    "        \"mae\",\n",
    "        \"smape\",\n",
    "        \"r2_score\",\n",
    "        \"spearman\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict[\"spearman\"][\"Our Model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics_by_prediction_length(\n",
    "#     all_metrics_dict[\"spearman\"], \"Spearman\", show_std_envelope=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict[\"mse\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = [\"o\", \"s\", \"v\", \"D\", \"X\"]\n",
    "\n",
    "\n",
    "def plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict: dict[str, dict[str, dict[str, list[float]]]],\n",
    "    metric_names: list[str],\n",
    "    metrics_to_show_std_envelope: list[str],\n",
    "    n_rows: int = 2,\n",
    "    n_cols: int = 3,\n",
    "    save_path: str | None = None,\n",
    "):\n",
    "    num_metrics = len(metric_names)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_rows, ncols=n_cols, figsize=(4 * n_cols, 4 * n_rows)\n",
    "    )\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "    for i, (ax, metric_name) in enumerate(zip(axes, metric_names)):\n",
    "        metrics_dict = all_metrics_dict[metric_name]\n",
    "        for j, (model_name, metrics) in enumerate(metrics_dict.items()):\n",
    "            # print(model_name)\n",
    "            # print(metrics.keys())\n",
    "            ax.plot(\n",
    "                metrics[\"prediction_lengths\"],\n",
    "                metrics[\"medians\"],\n",
    "                marker=markers[j],\n",
    "                label=model_name,\n",
    "                markersize=6,\n",
    "                # alpha=0.8,\n",
    "            )\n",
    "            # std_envelope = np.array(metrics[\"stds\"])\n",
    "            se_envelope = np.array(metrics[\"stds\"]) / np.sqrt(len(metrics[\"stds\"]))\n",
    "            if metric_name in metrics_to_show_std_envelope:\n",
    "                ax.fill_between(\n",
    "                    metrics[\"prediction_lengths\"],\n",
    "                    metrics[\"means\"] - se_envelope,\n",
    "                    metrics[\"means\"] + se_envelope,\n",
    "                    alpha=0.1,\n",
    "                )\n",
    "        if i == 0:\n",
    "            ax.legend(loc=\"upper left\", frameon=True, fontsize=10)\n",
    "        ax.set_xlabel(\"Prediction Length\", fontweight=\"bold\", fontsize=12)\n",
    "        ax.set_xticks(metrics[\"prediction_lengths\"])\n",
    "        name = metric_name.replace(\"_\", \" \")\n",
    "        if name in [\"mse\", \"mae\", \"rmse\", \"mape\"]:\n",
    "            name = name.upper()\n",
    "        elif name == \"smape\":\n",
    "            name = \"sMAPE\"\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        ax.set_title(name, fontweight=\"bold\", fontsize=16)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for ax in axes[num_metrics:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    metrics_to_show_std_envelope=[\"smape\", \"pearson\", \"spearman\"],\n",
    "    n_cols=4,\n",
    "    n_rows=1,\n",
    "    save_path=\"zeroshot_metrics_autoregressive_rollout_metrics.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make bar plot of mse and smape for each model at prediction length 128\n",
    "# make bar plot of mse and smape for each model at prediction length 128\n",
    "# smape_metrics_dict = get_summary_metrics_dict(unrolled_metrics, \"smape\")\n",
    "\n",
    "colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.bar(\n",
    "    [\"Chronos Finetune\", \"Ours\"],\n",
    "    [\n",
    "        np.median(unrolled_metrics[\"Chronos 20M Finetune\"][128][\"smape\"]),\n",
    "        np.median(unrolled_metrics[\"Our Model\"][128][\"smape\"]),\n",
    "    ],\n",
    "    color=[colors[0], colors[1]],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
