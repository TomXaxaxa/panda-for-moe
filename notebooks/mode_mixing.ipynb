{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from dystformer.patchtst.pipeline import PatchTSTPipeline\n",
    "from dystformer.utils import apply_custom_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply matplotlib style from config\n",
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = \"pft_stand_rff_univariate-0\"\n",
    "# run_name = \"pft_chattn_emb_w_poly-0\"\n",
    "# run_name = \"pft_linattn_noemb_from_scratch-0\"\n",
    "run_name = \"pft_chattn_fullemb_pretrained-0\"\n",
    "pft_model = PatchTSTPipeline.from_pretrained(\n",
    "    mode=\"predict\",\n",
    "    pretrain_path=f\"/stor/work/AMDG_Gilpin_Summer2024/checkpoints/{run_name}/checkpoint-final\",\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SineND:\n",
    "    freqs: list[float]\n",
    "    ts: np.ndarray\n",
    "\n",
    "    def __call__(self, s: float):\n",
    "        return np.array([np.sin(2 * np.pi * s * f * self.ts) for f in self.freqs])\n",
    "\n",
    "\n",
    "ts = np.linspace(0, 1, 4096)\n",
    "incomm_fn = SineND([1.0, np.pi, np.pi**2], ts)\n",
    "comm_fn = SineND([1, 3, 9], ts)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "gs = fig.add_gridspec(\n",
    "    2, 2, height_ratios=[0.5, 0.5], width_ratios=[0.4, 0.6], wspace=0.2, hspace=0\n",
    ")\n",
    "ax = fig.add_subplot(gs[:, 0], projection=\"3d\")\n",
    "ax.plot3D(*incomm_fn(1), color=\"blue\", label=\"incomm\")\n",
    "ax.plot3D(*comm_fn(1), color=\"orange\", label=\"comm\")\n",
    "ax.legend()\n",
    "for i, d in enumerate([incomm_fn(1), comm_fn(1)]):\n",
    "    ax = fig.add_subplot(gs[i, 1])\n",
    "    for j in range(3):\n",
    "        ax.plot(ts, d[j], color=[\"blue\", \"orange\"][i])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_attn_map_sequence(\n",
    "    model,\n",
    "    series_generator,\n",
    "    series_params: np.ndarray,\n",
    "    context_length: int,\n",
    "    channel_idx: int,\n",
    "    head_idx: int,\n",
    "    patch_size: int,\n",
    "    colormap=\"magma\",\n",
    "    save_path: str | None = None,\n",
    "    fps: int = 30,\n",
    "    linear_attn: bool = False,\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from IPython.display import HTML\n",
    "    from matplotlib.animation import FuncAnimation\n",
    "    from matplotlib.gridspec import GridSpec\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 7))\n",
    "    gs = GridSpec(3, 4, figure=fig, height_ratios=[1, 2, 2])\n",
    "\n",
    "    ax_ts = fig.add_subplot(gs[0, :])\n",
    "    ax_ts.set_xlim(0, 1)\n",
    "    ax_ts.set_xticks([])\n",
    "    ax_ts.set_yticks([])\n",
    "    (line,) = ax_ts.plot([], [])  # Empty line for time series\n",
    "\n",
    "    # Create attention map axes\n",
    "    axes = []\n",
    "    im_list = []\n",
    "    for i in range(8):\n",
    "        row = (i // 4) + 1  # Start from row 1 (after time series)\n",
    "        col = i % 4\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "        im = ax.imshow(np.zeros((context_length, context_length)), cmap=colormap)\n",
    "        ax.set_axis_off()\n",
    "        ax.set_title(f\"Layer {2 * i}\")\n",
    "        axes.append(ax)\n",
    "        im_list.append(im)\n",
    "\n",
    "    attnmaps = np.zeros(\n",
    "        (len(series_params), 8, context_length // 16, context_length // 16)\n",
    "    )\n",
    "    ts = np.linspace(0, 1, context_length + 128)\n",
    "\n",
    "    def update(frame):\n",
    "        param = series_params[frame]\n",
    "        series = series_generator(param)\n",
    "        context = series[:, :context_length]\n",
    "\n",
    "        context_tensor = torch.from_numpy(context.T).float().to(model.device)[None, ...]\n",
    "        pred = model(context_tensor, output_attentions=True, linear_attn=linear_attn)\n",
    "        attn_weights = pred.attentions\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            attnmap = (\n",
    "                attn_weights[(1 if len(attn_weights) == 8 else 2) * i][\n",
    "                    channel_idx, head_idx\n",
    "                ]\n",
    "                .detach()\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "            attnmaps[frame, i, :, :] = attnmap\n",
    "            im_list[i].set_array(attnmap)\n",
    "            vmin, vmax = attnmap.min(), attnmap.max()\n",
    "            im_list[i].set_clim(vmin, vmax)\n",
    "\n",
    "        # plot context and prediction\n",
    "        pred_channel = pred.prediction_outputs[0, :, channel_idx].detach().cpu().numpy()\n",
    "        ax_ts.clear()\n",
    "\n",
    "        for i in range(1, context_length // patch_size + 1):\n",
    "            boundary = i * patch_size\n",
    "            if boundary < context_length + 1:\n",
    "                ax_ts.axvline(\n",
    "                    x=float(ts[boundary]), color=\"gray\", linestyle=\":\", alpha=0.5\n",
    "                )\n",
    "\n",
    "        ax_ts.plot(ts[:context_length], context[channel_idx], color=\"black\")\n",
    "        ax_ts.plot(\n",
    "            ts[context_length:],\n",
    "            series[channel_idx, context_length : context_length + 128],\n",
    "            color=\"black\",\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "        ax_ts.plot(ts[context_length:], pred_channel, color=\"red\")\n",
    "        ax_ts.set_xlim(0, 1)\n",
    "        ax_ts.set_xticks([])\n",
    "        ax_ts.set_yticks([])\n",
    "        ax_ts.set_title(\n",
    "            f\"Input Time Series head {head_idx} channel {channel_idx} param={param:.3f}\"\n",
    "        )\n",
    "\n",
    "        return im_list\n",
    "\n",
    "    anim = FuncAnimation(\n",
    "        fig,\n",
    "        update,\n",
    "        frames=len(series_params),\n",
    "        interval=1000 / fps,\n",
    "        blit=True,\n",
    "    )\n",
    "\n",
    "    if save_path is not None:\n",
    "        anim.save(\n",
    "            save_path,\n",
    "            writer=\"pillow\" if save_path.endswith(\".gif\") else \"ffmpeg\",\n",
    "            fps=fps,\n",
    "        )\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.close()\n",
    "        display(HTML(anim.to_jshtml()))\n",
    "\n",
    "    return attnmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx = 2\n",
    "channel_idx = 2\n",
    "\n",
    "params = np.linspace(20, 30, 50)\n",
    "attnmaps = temporal_attn_map_sequence(\n",
    "    pft_model.model,\n",
    "    incomm_fn,\n",
    "    params,\n",
    "    context_length=512,\n",
    "    channel_idx=channel_idx,\n",
    "    head_idx=head_idx,\n",
    "    patch_size=16,\n",
    "    fps=5,\n",
    "    save_path=f\"../figures/interp_{run_name}_{channel_idx}_{head_idx}.mp4\",\n",
    "    linear_attn=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_attn_maps(\n",
    "    model,\n",
    "    series: np.ndarray,\n",
    "    context_length: int,\n",
    "    linear_attn: bool = False,\n",
    "):\n",
    "    context = series[:, :context_length]\n",
    "    if context.ndim == 1:\n",
    "        context = context[None, ...]\n",
    "    context_tensor = torch.from_numpy(context).float().to(model.device)\n",
    "    pred = model(\n",
    "        context_tensor[:, -context_length:, :],\n",
    "        output_attentions=True,\n",
    "        linear_attn=linear_attn,\n",
    "    )\n",
    "    return pred.attentions\n",
    "\n",
    "\n",
    "def interaction_index(\n",
    "    matrix: np.ndarray, axes: tuple[int, int] = (-2, -1)\n",
    ") -> np.ndarray:\n",
    "    fronorm = np.linalg.norm(matrix, axis=axes, ord=\"fro\")\n",
    "    twonorm = np.linalg.norm(matrix, axis=axes, ord=2)\n",
    "    return (fronorm - twonorm) / (fronorm + 1e-10)\n",
    "\n",
    "\n",
    "def mean_row_entropy(\n",
    "    matrix: np.ndarray, axis: int = -1, eps: float = 1e-10\n",
    ") -> np.ndarray:\n",
    "    assert np.allclose(matrix.sum(axis=axis), 1), (\n",
    "        \"All rows must be a probability distribution\"\n",
    "    )\n",
    "    return -np.sum(matrix * np.log(matrix + eps), axis=axis).mean(axis=axis)\n",
    "\n",
    "\n",
    "def fronorm(matrix: np.ndarray, axes: tuple[int, int] = (-2, -1)) -> np.ndarray:\n",
    "    return np.linalg.norm(matrix, axis=axes, ord=\"fro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TwoToneSine:\n",
    "    freqs1: np.ndarray\n",
    "    freqs2: np.ndarray\n",
    "    base_freq: float\n",
    "    ts: np.ndarray\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.indices = np.arange(self.freqs1.shape[0] * self.freqs2.shape[0])\n",
    "        self.basewave = np.sin(2 * np.pi * self.base_freq * self.ts)\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple[int, int]:\n",
    "        return self.freqs1.shape[0], self.freqs2.shape[0]\n",
    "\n",
    "    @property\n",
    "    def dims(self) -> int:\n",
    "        return 3\n",
    "\n",
    "    def product_array_indices(self, s: slice) -> tuple[np.ndarray, ...]:\n",
    "        return np.unravel_index(\n",
    "            self.indices[s], (self.freqs1.shape[0], self.freqs2.shape[0])\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.freqs1.shape[0] * self.freqs2.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int | slice) -> np.ndarray:\n",
    "        if isinstance(idx, int):\n",
    "            i = idx % self.freqs1.shape[0]\n",
    "            j = idx // self.freqs1.shape[0]\n",
    "            return np.array(\n",
    "                [\n",
    "                    np.sin(2 * np.pi * self.freqs1[i] * self.ts),\n",
    "                    np.sin(2 * np.pi * self.freqs2[j] * self.ts),\n",
    "                    np.sin(2 * np.pi * self.base_freq * self.ts),\n",
    "                ]\n",
    "            ).T\n",
    "        elif isinstance(idx, slice):\n",
    "            idxi, idxj = self.product_array_indices(idx)\n",
    "            return np.stack(\n",
    "                [\n",
    "                    np.sin(2 * np.pi * self.freqs1[idxi, None] * self.ts),\n",
    "                    np.sin(2 * np.pi * self.freqs2[idxj, None] * self.ts),\n",
    "                    self.basewave[None, :].repeat(len(idxi), axis=0),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bispectra_fpath = \"bispectra.npy\"\n",
    "\n",
    "if os.path.exists(bispectra_fpath):\n",
    "    bispectra = np.load(bispectra_fpath)\n",
    "else:\n",
    "    resolution = 1024\n",
    "    freqs = np.linspace(1, 50, resolution)\n",
    "    series_fn = TwoToneSine(freqs, freqs, 10, np.linspace(0, 1, 1024))\n",
    "\n",
    "    bispectra = np.zeros((8, resolution * resolution, 3, 8))\n",
    "    channel_idx = 1\n",
    "    head_idx = 7\n",
    "    batch_size = 2048\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    for batch in trange(resolution * resolution // batch_size):\n",
    "        series_batch = series_fn[batch * batch_size : (batch + 1) * batch_size]\n",
    "        attn_weights = extract_attn_maps(\n",
    "            pft_model.model,\n",
    "            series_batch,\n",
    "            512,\n",
    "            linear_attn=False,\n",
    "        )\n",
    "        for i in range(8):\n",
    "            attnmap = attn_weights[2 * i].detach().cpu().numpy()\n",
    "            attnmap = attnmap.reshape(batch_size, 3, 8, 32, 32)\n",
    "            response = fronorm(attnmap)\n",
    "            bispectra[i, batch * batch_size : (batch + 1) * batch_size] = response\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bispectra = bispectra.reshape(-1, resolution, resolution, 3, 8)\n",
    "\n",
    "    np.save(\"bispectra.npy\", bispectra)\n",
    "\n",
    "print(bispectra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_idx = 0\n",
    "head_idx = 2\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "for i in range(8):\n",
    "    axes[i // 4, i % 4].imshow(\n",
    "        bispectra[i, :, :, channel_idx, head_idx], origin=\"lower\"\n",
    "    )\n",
    "    axes[i // 4, i % 4].set_title(f\"Layer {2 * (i + 1)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bispectra_scaling(\n",
    "    series_fn,\n",
    "    checkpoint_dir: str,\n",
    "    context_length: int = 512,\n",
    "    batch_size: int = 2048,\n",
    "    dims: int | None = None,\n",
    "    norm: Callable = fronorm,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Plots the bispectra across all checkpoints in the given directory.\n",
    "\n",
    "    Assumes that the checkpoint directory contains checkpoint-{i} folders for i in [0, 1, 2, ...]\n",
    "    and a single checkpoint-final folder.\n",
    "    \"\"\"\n",
    "    if dims is None:\n",
    "        assert hasattr(series_fn, \"dims\"), (\n",
    "            \"series_fn must have a dims attribute if dims is not provided\"\n",
    "        )\n",
    "    dims = dims or series_fn.dims\n",
    "\n",
    "    H, W = series_fn.shape\n",
    "    checkpoints = os.listdir(checkpoint_dir)\n",
    "    checkpoints.remove(\"checkpoint-final\")\n",
    "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
    "\n",
    "    max_interactions = []\n",
    "    for i, checkpoint in enumerate(tqdm(checkpoints, desc=\"Processing checkpoints\")):\n",
    "        model = PatchTSTPipeline.from_pretrained(\n",
    "            mode=\"predict\",\n",
    "            pretrain_path=f\"{checkpoint_dir}/{checkpoint}\",\n",
    "            device_map=\"cuda:0\",\n",
    "        )\n",
    "\n",
    "        batch_size = min(batch_size, H * W)\n",
    "        num_heads = model.model.config.num_attention_heads\n",
    "        num_tokens = context_length // model.model.config.patch_length\n",
    "        num_layers = model.model.config.num_hidden_layers\n",
    "\n",
    "        attnmap_shape = (batch_size, dims, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        bispectra = np.zeros((num_layers, H * W, dims, num_heads))\n",
    "\n",
    "        for batch in range(H * W // batch_size):\n",
    "            attn_weights = extract_attn_maps(\n",
    "                model.model,\n",
    "                series_fn[batch * batch_size : (batch + 1) * batch_size],\n",
    "                context_length,\n",
    "                linear_attn=False,\n",
    "            )\n",
    "            for i in range(num_layers):\n",
    "                attnmap = (\n",
    "                    attn_weights[2 * i].detach().cpu().numpy().reshape(attnmap_shape)\n",
    "                )\n",
    "                response = norm(attnmap)\n",
    "                bispectra[i, batch * batch_size : (batch + 1) * batch_size] = response\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # shape: (num_layers, H, W, dims, num_heads)\n",
    "        bispectra = bispectra.reshape(-1, H, W, dims, num_heads)\n",
    "        interactions = interaction_index(bispectra, axes=(1, 2)).mean(axis=(-1, -2))\n",
    "        max_interactions.append(interactions)\n",
    "\n",
    "        # cleanup manually\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.array(max_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = np.linspace(1, 40, 32)\n",
    "series_fn = TwoToneSine(freqs, freqs, 10, np.linspace(0, 1, 1024))\n",
    "workdir = os.environ[\"WORK\"]\n",
    "interactions = bispectra_scaling(\n",
    "    series_fn,\n",
    "    # f\"{workdir}/checkpoints/pft_chattn_fullemb_pretrained-0\",\n",
    "    f\"{workdir}/checkpoints/pft_chattn_mlm_sys5245_ic4-0\",\n",
    "    # f\"{workdir}/checkpoints/pft_chattn_mlm_sys164_ic128-0\",\n",
    "    norm=fronorm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(interactions.T, cmap=\"magma\", aspect=\"auto\", origin=\"lower\")\n",
    "plt.ylabel(\"Layer (channel attention)\")\n",
    "plt.xlabel(\"Checkpoint\")\n",
    "plt.yticks(np.arange(8), np.arange(2, 2 * 8 + 2, 2))\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CoupledOscillator:\n",
    "    num_oscillators: int\n",
    "    mass: float = 1.0\n",
    "    spring_constant: float = 1.0\n",
    "    w0: float = 1.0\n",
    "    initial_conditions: np.ndarray | None = None\n",
    "\n",
    "    tspan: tuple[float, float] = (0, 100)\n",
    "    num_eval_points: int = 1000\n",
    "    k_function: Callable = lambda x: 1.0\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.stencil = np.zeros((self.num_oscillators, self.num_oscillators))\n",
    "        for i in range(self.num_oscillators):\n",
    "            self.stencil[i, (i - 1) % self.num_oscillators] = 1\n",
    "            self.stencil[i, (i + 1) % self.num_oscillators] = 1\n",
    "            self.stencil[i, i] = -2\n",
    "        self.stencil /= self.mass\n",
    "\n",
    "        if self.initial_conditions is None:\n",
    "            self.initial_conditions = np.random.randn(2 * self.dim)\n",
    "\n",
    "        self.ts = np.linspace(self.tspan[0], self.tspan[1], self.num_eval_points)\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self.num_oscillators\n",
    "\n",
    "    def __call__(self, t: float, uv: np.ndarray) -> np.ndarray:\n",
    "        u, v = uv[: self.dim], uv[self.dim :]\n",
    "        dudt = v\n",
    "        dvdt = -(self.w0**2) * u + self.spring_constant * self.stencil @ u\n",
    "        return np.concatenate([dudt, dvdt])\n",
    "\n",
    "    def integrate(self) -> np.ndarray:\n",
    "        sol = solve_ivp(self, self.tspan, self.initial_conditions, t_eval=self.ts)\n",
    "        return sol.y[: self.dim].T\n",
    "\n",
    "    def __getitem__(self, idx: int | slice) -> np.ndarray:\n",
    "        if isinstance(idx, int):\n",
    "            self.spring_constant = self.k_function(idx)\n",
    "            return self.integrate()\n",
    "        elif isinstance(idx, slice):\n",
    "            inds = np.arange(idx.start, idx.stop, idx.step)\n",
    "            solutions = np.zeros((len(inds), self.num_eval_points, self.dim))\n",
    "            for i, ind in tqdm(enumerate(inds), total=len(inds)):\n",
    "                self.spring_constant = self.k_function(ind)\n",
    "                solutions[i] = self.integrate()\n",
    "            return solutions\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid index: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout(attention_stack, skip_connection=True):\n",
    "    \"\"\"\n",
    "    Computes the attention rollout for a stack of attention matrices.\n",
    "    Based on the description in Abnar & Zuidema\n",
    "    https://arxiv.org/pdf/2005.00928\n",
    "\n",
    "    Args:\n",
    "        attention_stack (torch.Tensor): Tensor of shape (L, *, C, C) containing L attention\n",
    "            matrices.\n",
    "        skip_connection (bool): If True, adds an identity matrix to each attention matrix\n",
    "            to account for residual connections.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A (*, C, C) rollout attention matrix.\n",
    "    \"\"\"\n",
    "    L, *_, C, _ = attention_stack.shape\n",
    "    rollout = torch.eye(C, device=attention_stack.device)[None, ...]\n",
    "    for i in range(L):\n",
    "        A = attention_stack[i]\n",
    "        if skip_connection:\n",
    "            A = 0.5 * (A + torch.eye(C, device=A.device))\n",
    "            A = A / A.sum(dim=-1, keepdim=True)\n",
    "        rollout = A @ rollout\n",
    "    return rollout\n",
    "\n",
    "\n",
    "def single_head_attn_rollout(\n",
    "    model, data, context_length: int = 512, attention_type: str = \"temporal\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the attention rollout for the whole model by averaging over heads.\n",
    "    \"\"\"\n",
    "    bs, _, num_channels = data.shape\n",
    "    attn_weights = extract_attn_maps(\n",
    "        model.model,\n",
    "        data,\n",
    "        context_length,\n",
    "        linear_attn=False,\n",
    "    )\n",
    "    if attention_type == \"channel\":\n",
    "        attn_weights = attn_weights[1::2]\n",
    "    elif attention_type == \"temporal\":\n",
    "        attn_weights = attn_weights[0::2]\n",
    "    else:\n",
    "        raise ValueError(\"Attention type must be either 'channel' or 'temporal'\")\n",
    "\n",
    "    num_layers = len(attn_weights)\n",
    "    num_tokens = context_length // model.model.config.patch_length\n",
    "\n",
    "    # average over heads\n",
    "    # if attention_type is channel\n",
    "    # shape: (num_layers, batch_size*num_tokens, num_channels, num_channels)\n",
    "    # if attention_type is temporal\n",
    "    # shape: (num_layers, batch_size*num_channels, num_tokens, num_tokens)\n",
    "    attn_weights = torch.stack(attn_weights, dim=0).mean(dim=2)\n",
    "\n",
    "    n = num_tokens if attention_type == \"temporal\" else num_channels\n",
    "    m = num_channels if attention_type == \"temporal\" else num_tokens\n",
    "    return (\n",
    "        attention_rollout(attn_weights).detach().cpu().numpy()\n",
    "    )  # .reshape(bs, m, n, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_fn = CoupledOscillator(num_oscillators=6, mass=1.0, spring_constant=1, w0=1.0)\n",
    "series_fn.k_function = lambda i: (np.pi / 2) ** (i - 18)\n",
    "data = series_fn[0:36]\n",
    "\n",
    "attn_type = \"temporal\"\n",
    "\n",
    "rollouts = single_head_attn_rollout(\n",
    "    pft_model.model,\n",
    "    data,\n",
    "    context_length=512,\n",
    "    attention_type=attn_type,\n",
    ")\n",
    "rollouts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 6, figsize=(20, 20))\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "\n",
    "inds = np.random.randint(0, rollouts.shape[0], size=data.shape[0])\n",
    "# inds = np.arange(\n",
    "#     5,\n",
    "#     rollouts.shape[0],\n",
    "#     series_fn.dim if attn_type == \"temporal\" else len(ts) // pft_model.model.config.patch_length,\n",
    "# )\n",
    "\n",
    "for ax, ind in zip(axes.ravel(), inds):\n",
    "    ax.imshow(rollouts[ind], cmap=\"magma\")\n",
    "    ax.set_axis_off()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer_jeff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
