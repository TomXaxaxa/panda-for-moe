{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.integrate import solve_ivp\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from dystformer.patchtst.pipeline import PatchTSTPipeline\n",
    "from dystformer.utils import safe_standardize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = \"pft_stand_rff_univariate-0\"\n",
    "run_name = \"pft_chattn_emb_w_poly-0\"\n",
    "# run_name = \"pft_linattn_noemb_from_scratch-0\"\n",
    "# run_name = \"pft_chattn_fullemb_pretrained-0\"\n",
    "pft_model = PatchTSTPipeline.from_pretrained(\n",
    "    mode=\"predict\",\n",
    "    pretrain_path=f\"/stor/work/AMDG_Gilpin_Summer2024/checkpoints/{run_name}/checkpoint-final\",\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_attn_maps(\n",
    "    model,\n",
    "    series: np.ndarray,\n",
    "    context_length: int,\n",
    "    linear_attn: bool = False,\n",
    "):\n",
    "    context = series[:, :context_length]\n",
    "    if context.ndim == 1:\n",
    "        context = context[None, ...]\n",
    "    context_tensor = torch.from_numpy(context).float().to(model.device)\n",
    "    pred = model(\n",
    "        context_tensor[:, -context_length:, :],\n",
    "        output_attentions=True,\n",
    "        linear_attn=linear_attn,\n",
    "    )\n",
    "    return pred.attentions\n",
    "\n",
    "\n",
    "def show_forecast(\n",
    "    model,\n",
    "    data: np.ndarray,\n",
    "    context_length: int,\n",
    "    prediction_length: int,\n",
    "    transient_length: int = 1024,\n",
    "    **kwargs,\n",
    "):\n",
    "    data = data[:, transient_length:, :]\n",
    "\n",
    "    context_data = data[:, :context_length, :]\n",
    "    stand_context = safe_standardize(context_data, axis=1)\n",
    "    predictions = (\n",
    "        model.predict(\n",
    "            torch.from_numpy(stand_context).float(),\n",
    "            prediction_length=prediction_length,\n",
    "            **kwargs,\n",
    "        )[:, 0, ...]\n",
    "        .detach()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    stand_gt = safe_standardize(\n",
    "        data[:, context_length : context_length + prediction_length, :],\n",
    "        context=data[:, : context_length + prediction_length, :],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    context_ts = np.arange(0, context_length)\n",
    "    prediction_ts = np.arange(context_length, context_length + prediction_length)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i in range(data.shape[-1]):\n",
    "        plt.plot(context_ts, stand_context[0, :, i], color=\"k\", alpha=0.5)\n",
    "        plt.plot(prediction_ts, stand_gt[0, :, i], color=\"k\", linestyle=\"--\", alpha=0.3)\n",
    "        plt.plot(prediction_ts, predictions[0, :, i], color=\"r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def interaction_index(\n",
    "    matrix: np.ndarray, axes: tuple[int, int] = (-2, -1)\n",
    ") -> np.ndarray:\n",
    "    fronorm = np.linalg.norm(matrix, axis=axes, ord=\"fro\")\n",
    "    twonorm = np.linalg.norm(matrix, axis=axes, ord=2)\n",
    "    return (fronorm - twonorm) / (fronorm + 1e-10)\n",
    "\n",
    "\n",
    "def mean_row_entropy(\n",
    "    matrix: np.ndarray, axis: int = -1, eps: float = 1e-10\n",
    ") -> np.ndarray:\n",
    "    assert np.allclose(matrix.sum(axis=axis), 1), (\n",
    "        \"All rows must be a probability distribution\"\n",
    "    )\n",
    "    return -np.sum(matrix * np.log(matrix + eps), axis=axis).mean(axis=axis)\n",
    "\n",
    "\n",
    "def fronorm(matrix: np.ndarray, axes: tuple[int, int] = (-2, -1)) -> np.ndarray:\n",
    "    return np.linalg.norm(matrix, axis=axes, ord=\"fro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout(\n",
    "    attention_stack, skip_connection=True, start_layer=-1, stop_layer=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the attention rollout for a stack of attention matrices.\n",
    "    Based on the description in Abnar & Zuidema\n",
    "    https://arxiv.org/pdf/2005.00928\n",
    "\n",
    "    Args:\n",
    "        attention_stack (torch.Tensor): Tensor of shape (L, *, C, C) containing L attention\n",
    "            matrices.\n",
    "        skip_connection (bool): If True, adds an identity matrix to each attention matrix\n",
    "            to account for residual connections.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A (*, C, C) rollout attention matrix.\n",
    "    \"\"\"\n",
    "    L, *_, C, _ = attention_stack.shape\n",
    "    rollouts = torch.zeros(\n",
    "        L + 1, *attention_stack.shape[1:], device=attention_stack.device\n",
    "    )\n",
    "    rollout = torch.eye(C, device=attention_stack.device)[None, ...]\n",
    "    rollouts[0] = rollout\n",
    "    for i in range(L):\n",
    "        A = attention_stack[i]\n",
    "        if skip_connection:\n",
    "            A = 0.5 * (A + torch.eye(C, device=A.device))\n",
    "            A = A / A.sum(dim=-1, keepdim=True)\n",
    "        rollout = A @ rollout\n",
    "        rollouts[i + 1] = rollout\n",
    "    return rollouts[start_layer:stop_layer]\n",
    "\n",
    "\n",
    "def single_head_attn_rollout(\n",
    "    model,\n",
    "    data: np.ndarray,\n",
    "    context_length: int = 512,\n",
    "    attention_type: str = \"temporal\",\n",
    "    **kwargs,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the attention rollout for the whole model by averaging over heads.\n",
    "    \"\"\"\n",
    "    bs, _, num_channels = data.shape\n",
    "    attn_weights = extract_attn_maps(\n",
    "        model.model,\n",
    "        data,\n",
    "        context_length,\n",
    "        linear_attn=False,\n",
    "    )\n",
    "    if attention_type == \"channel\":\n",
    "        attn_weights = attn_weights[1::2]\n",
    "    elif attention_type == \"temporal\":\n",
    "        attn_weights = attn_weights[0::2]\n",
    "    else:\n",
    "        raise ValueError(\"Attention type must be either 'channel' or 'temporal'\")\n",
    "\n",
    "    # average over heads\n",
    "    # if attention_type is channel\n",
    "    # shape: (num_layers, batch_size*num_tokens, num_channels, num_channels)\n",
    "    # if attention_type is temporal\n",
    "    # shape: (num_layers, batch_size*num_channels, num_tokens, num_tokens)\n",
    "    attn_weights = torch.stack(attn_weights, dim=0).mean(dim=2)\n",
    "\n",
    "    num_tokens = context_length // model.model.config.patch_length\n",
    "    n = num_tokens if attention_type == \"temporal\" else num_channels\n",
    "    m = num_channels if attention_type == \"temporal\" else num_tokens\n",
    "    return (attention_rollout(attn_weights, **kwargs).detach().cpu().numpy()).reshape(\n",
    "        -1, bs, m, n, n\n",
    "    )\n",
    "\n",
    "\n",
    "def multi_head_attn_rollout(\n",
    "    model, data: np.ndarray, context_length: int = 512, attention_type: str = \"temporal\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute attention rollout for each head by averaging upstream heads\"\"\"\n",
    "    bs, _, num_channels = data.shape\n",
    "    attn_weights = extract_attn_maps(\n",
    "        model.model,\n",
    "        data,\n",
    "        context_length,\n",
    "        linear_attn=False,\n",
    "    )\n",
    "    if attention_type == \"channel\":\n",
    "        attn_weights = attn_weights[1::2]\n",
    "    elif attention_type == \"temporal\":\n",
    "        attn_weights = attn_weights[0::2]\n",
    "    else:\n",
    "        raise ValueError(\"Attention type must be either 'channel' or 'temporal'\")\n",
    "\n",
    "    # shape: (num_layers, batch_size*num_channels, num_heads, num_tokens, num_tokens)\n",
    "    attn_weights = torch.stack(attn_weights, dim=0)\n",
    "    L, S, H, C, _ = attn_weights.shape\n",
    "\n",
    "    # shape: (num_layers, bs, num_tokens, num_tokens)\n",
    "    single_head_rollouts = attention_rollout(\n",
    "        attn_weights.mean(dim=2), start_layer=0, stop_layer=L\n",
    "    )\n",
    "    # shape: (num_layers, bs, num_heads, num_tokens, num_tokens)\n",
    "    rollouts = attn_weights @ single_head_rollouts.unsqueeze(2)\n",
    "\n",
    "    num_tokens = context_length // model.model.config.patch_length\n",
    "    n = num_tokens if attention_type == \"temporal\" else num_channels\n",
    "    m = num_channels if attention_type == \"temporal\" else num_tokens\n",
    "\n",
    "    # shape: (num_layers, batch_size, channels or tokens, num_heads, tokens or channels, tokens or channels)\n",
    "    return rollouts.reshape(L, bs, m, H, n, n).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CoupledOscillator:\n",
    "    num_oscillators: int\n",
    "    w0: float = 1.0\n",
    "\n",
    "    initial_conditions: np.ndarray | None = None\n",
    "    spring_constants: np.ndarray | None = None\n",
    "    masses: np.ndarray | None = None\n",
    "    stencil: np.ndarray | None = None\n",
    "\n",
    "    tspan: tuple[float, float] = (0, 100)\n",
    "    num_eval_points: int = 1024\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.spring_constants is None:\n",
    "            self.spring_constants = np.ones(self.num_oscillators)\n",
    "\n",
    "        if self.masses is None:\n",
    "            self.masses = np.ones(self.num_oscillators)\n",
    "\n",
    "        if self.initial_conditions is None:\n",
    "            self.initial_conditions = np.random.randn(2 * self.dim)\n",
    "            self.initial_conditions[self.dim :] = 0\n",
    "\n",
    "        if self.stencil is None:\n",
    "            self.stencil = np.zeros((self.num_oscillators, self.num_oscillators))\n",
    "            for i in range(self.num_oscillators):\n",
    "                self.stencil[i, (i - 1) % self.num_oscillators] = 1\n",
    "                self.stencil[i, (i + 1) % self.num_oscillators] = 1\n",
    "                self.stencil[i, i] = -2\n",
    "                self.stencil *= self.spring_constants[i] / self.masses[i]\n",
    "\n",
    "        self.ts = np.linspace(self.tspan[0], self.tspan[1], self.num_eval_points)\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self.num_oscillators\n",
    "\n",
    "    @property\n",
    "    def basis(self) -> np.ndarray:\n",
    "        return np.linalg.eigh(self.spring_constant * self.stencil)\n",
    "\n",
    "    def __call__(self, t: float, uv: np.ndarray) -> np.ndarray:\n",
    "        u, v = uv[: self.dim], uv[self.dim :]\n",
    "        dudt = v\n",
    "        dvdt = self.stencil @ u\n",
    "        return np.concatenate([dudt, dvdt])\n",
    "\n",
    "    def integrate(self) -> np.ndarray:\n",
    "        sol = solve_ivp(self, self.tspan, self.initial_conditions, t_eval=self.ts)\n",
    "        return sol.y[: self.dim].T\n",
    "\n",
    "    def __getitem__(self, idx: int | slice) -> np.ndarray:\n",
    "        if isinstance(idx, int):\n",
    "            return self.integrate()\n",
    "        elif isinstance(idx, slice):\n",
    "            inds = np.arange(idx.start, idx.stop, idx.step)\n",
    "            solutions = np.zeros((len(inds), self.num_eval_points, self.dim))\n",
    "            for i, ind in tqdm(enumerate(inds), total=len(inds)):\n",
    "                solutions[i] = self.integrate()\n",
    "            return solutions\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid index: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oscillators = 4\n",
    "masses = np.ones(num_oscillators) * 0.01\n",
    "springs = np.ones(num_oscillators) * 0.01\n",
    "\n",
    "masses[2] = 1000000\n",
    "\n",
    "series_fn = CoupledOscillator(\n",
    "    num_oscillators=num_oscillators,\n",
    "    masses=masses,\n",
    "    spring_constants=springs,\n",
    "    num_eval_points=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_forecast(\n",
    "    pft_model,\n",
    "    series_fn[0:1],\n",
    "    context_length=512,\n",
    "    prediction_length=512,\n",
    "    limit_prediction_length=False,\n",
    "    sliding_context=True,\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlehead_rollouts = single_head_attn_rollout(\n",
    "    pft_model.model,\n",
    "    series_fn[0:1],\n",
    "    context_length=1024,\n",
    "    attention_type=\"channel\",\n",
    "    start_layer=0,\n",
    "    stop_layer=None,\n",
    ")[:, 0, ...]\n",
    "singlehead_rollouts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idx = -1\n",
    "fig, axes = plt.subplots(\n",
    "    1, singlehead_rollouts.shape[0], figsize=(singlehead_rollouts.shape[0] * 2, 2)\n",
    ")\n",
    "plt.subplots_adjust(wspace=0.0)\n",
    "for i in range(singlehead_rollouts.shape[0]):\n",
    "    axes[i].imshow(singlehead_rollouts[i, token_idx], cmap=\"magma\")\n",
    "    axes[i].set_axis_off()\n",
    "fig.supxlabel(\"Layer\", fontsize=20, y=-0.05, x=0.50);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_maps = extract_attn_maps(\n",
    "    pft_model.model,\n",
    "    series_fn[0:1],\n",
    "    context_length=1024,\n",
    "    linear_attn=False,\n",
    ")\n",
    "len(attn_maps), attn_maps[0].shape, attn_maps[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = -1\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.05)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        amap = attn_maps[2 * j + 1][sample_idx, i].detach().cpu().numpy()\n",
    "        axes[i, j].imshow(amap, cmap=\"magma\")\n",
    "        axes[i, j].set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_rollouts = multi_head_attn_rollout(\n",
    "    pft_model.model, series_fn[0:1], context_length=1024, attention_type=\"channel\"\n",
    ")[:, 0, ...]\n",
    "multihead_rollouts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idx = -1\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.05)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        amap = multihead_rollouts[j, token_idx, i]\n",
    "        axes[i, j].imshow(amap, cmap=\"magma\")\n",
    "        axes[i, j].set_axis_off()\n",
    "\n",
    "fig.supxlabel(\"Layer\", fontsize=20, y=0.08, x=0.51)\n",
    "fig.supylabel(\"Head\", fontsize=20, x=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sines:\n",
    "    num_waves: int\n",
    "    base_frequencies: np.ndarray\n",
    "    tspan: tuple[float, float] = (0, 100)\n",
    "    num_eval_points: int = 1024\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.ts = np.linspace(self.tspan[0], self.tspan[1], self.num_eval_points)\n",
    "\n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self.num_waves\n",
    "\n",
    "    def __getitem__(self, idx: int | slice) -> np.ndarray:\n",
    "        if isinstance(idx, int):\n",
    "            return np.stack([np.sin(self.base_frequencies * self.ts[:, None])], axis=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid index: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_fn = Sines(\n",
    "    num_waves=8,\n",
    "    base_frequencies=2 * np.pi * np.linspace(1 / 4, 1, 8),\n",
    "    num_eval_points=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_forecast(\n",
    "    pft_model,\n",
    "    series_fn[0],\n",
    "    context_length=512,\n",
    "    prediction_length=512,\n",
    "    limit_prediction_length=False,\n",
    "    sliding_context=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlehead_rollouts = single_head_attn_rollout(\n",
    "    pft_model.model,\n",
    "    series_fn[0],\n",
    "    context_length=1024,\n",
    "    attention_type=\"channel\",\n",
    "    start_layer=0,\n",
    "    stop_layer=None,\n",
    ")[:, 0, ...]\n",
    "singlehead_rollouts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idx = -1\n",
    "fig, axes = plt.subplots(\n",
    "    1, singlehead_rollouts.shape[0], figsize=(singlehead_rollouts.shape[0] * 2, 2)\n",
    ")\n",
    "plt.subplots_adjust(wspace=0.0)\n",
    "for i in range(singlehead_rollouts.shape[0]):\n",
    "    axes[i].imshow(singlehead_rollouts[i, token_idx], cmap=\"magma\")\n",
    "    axes[i].set_axis_off()\n",
    "fig.supxlabel(\"Layer\", fontsize=20, y=-0.05, x=0.50);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_maps = extract_attn_maps(\n",
    "    pft_model.model,\n",
    "    series_fn[0],\n",
    "    context_length=1024,\n",
    "    linear_attn=False,\n",
    ")\n",
    "len(attn_maps), attn_maps[0].shape, attn_maps[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = -1\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.05)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        amap = attn_maps[2 * j + 1][sample_idx, i].detach().cpu().numpy()\n",
    "        axes[i, j].imshow(amap, cmap=\"magma\")\n",
    "        axes[i, j].set_axis_off()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_rollouts = multi_head_attn_rollout(\n",
    "    pft_model.model, series_fn[0], context_length=1024, attention_type=\"channel\"\n",
    ")[:, 0, ...]\n",
    "multihead_rollouts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_idx = -1\n",
    "fig, axes = plt.subplots(8, 8, figsize=(10, 10))\n",
    "plt.subplots_adjust(wspace=0.0, hspace=0.05)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        amap = multihead_rollouts[j, token_idx, i]\n",
    "        axes[i, j].imshow(amap, cmap=\"magma\")\n",
    "        axes[i, j].set_axis_off()\n",
    "\n",
    "fig.supxlabel(\"Layer\", fontsize=20, y=0.08, x=0.51)\n",
    "fig.supylabel(\"Head\", fontsize=20, x=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer_jeff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
