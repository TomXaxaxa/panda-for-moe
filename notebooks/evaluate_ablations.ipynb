{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import colors as mcolors\n",
    "from matplotlib import patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../custom_style.mplstyle\"):\n",
    "    plt.style.use([\"ggplot\", \"../custom_style.mplstyle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../figures\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"final_skew40/test_zeroshot\"\n",
    "\n",
    "run_names_chattn = {\n",
    "    \"chattn + mlm + embedding\": \"pft_stand_rff_only_pretrained\",\n",
    "    # \"chattn + mlm + embedding\": \"pft_chattn_fullemb_pretrained\",\n",
    "    # \"chattn + mlm + embedding\": \"pft_chattn_fullemb_quartic_enc\",\n",
    "    \"chattn + mlm\": \"pft_chattn_noembed_pretrained_correct\",\n",
    "    \"chattn + embedding\": \"pft_fullyfeat_from_scratch\",\n",
    "    # \"chattn + embedding\": \"pft_chattn_emb_w_poly\",\n",
    "    # \"chattn + embedding\": \"pft_fullyfeat_from_scratch_longer\",\n",
    "    \"chattn\": \"pft_stand_chattn_noemb\",\n",
    "}\n",
    "\n",
    "run_names_no_chattn = {\n",
    "    # \"univariate (wider) + embedding\": \"pft_emb_equal_param_univariate_from_scratch\",\n",
    "    \"univariate (wider)\": \"pft_noemb_equal_param_univariate_from_scratch\",\n",
    "    \"univariate (deeper)\": \"pft_equal_param_deeper_univariate_from_scratch_noemb\",\n",
    "    # \"univariate + mlm + embedding\": \"pft_rff_univariate_pretrained\",\n",
    "    # \"univariate + mlm\": \"pft_vanilla_pretrained_correct\",\n",
    "    # \"tailmask\": \"pft_tailmasking_from_scratch\",\n",
    "}\n",
    "\n",
    "run_names = {\n",
    "    **run_names_chattn,\n",
    "    **run_names_no_chattn,\n",
    "}\n",
    "\n",
    "run_metrics_dirs_all_groups = {\n",
    "    \"chattn\": {\n",
    "        run_abbrv: os.path.join(\n",
    "            \"../eval_results\",\n",
    "            \"patchtst\",\n",
    "            f\"{run_name}-0\",\n",
    "            data_split,\n",
    "        )\n",
    "        for run_abbrv, run_name in run_names_chattn.items()\n",
    "    },\n",
    "    \"no_chattn\": {\n",
    "        run_abbrv: os.path.join(\n",
    "            \"../eval_results\",\n",
    "            \"patchtst\",\n",
    "            f\"{run_name}-0\",\n",
    "            data_split,\n",
    "        )\n",
    "        for run_abbrv, run_name in run_names_no_chattn.items()\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_names_to_use_sliding_context = {\n",
    "    \"chattn\": run_names_chattn.keys(),\n",
    "    \"no_chattn\": run_names_no_chattn.keys(),\n",
    "}\n",
    "for group, run_names in run_names_to_use_sliding_context.items():\n",
    "    for run_name in run_names:\n",
    "        curr_path = run_metrics_dirs_all_groups[group][run_name]\n",
    "        new_path = curr_path.replace(\"patchtst\", \"patchtst_sliding\")\n",
    "        run_metrics_dirs_all_groups[group][run_name] = new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dirs_all_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dirs_all_groups[\"no_chattn\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = defaultdict(lambda: defaultdict(dict))\n",
    "for run_group, run_metrics_dir_dict in run_metrics_dirs_all_groups.items():\n",
    "    print(f\"Run group: {run_group}\")\n",
    "    for run_abbrv, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "        run_abbrv = str(run_abbrv)\n",
    "        print(f\"{run_abbrv}: {run_metrics_dir}\")\n",
    "        for file in sorted(\n",
    "            os.listdir(run_metrics_dir),\n",
    "            key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "        ):\n",
    "            if file.endswith(\".csv\"):\n",
    "                prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "                # print(f\"Prediction length: {prediction_length} for {run_abbrv}\")\n",
    "                with open(os.path.join(run_metrics_dir, file), \"r\") as f:\n",
    "                    metrics = pd.read_csv(f).to_dict()\n",
    "                    metrics_all[run_group][run_abbrv][prediction_length] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all[\"no_chattn\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_groups = defaultdict(lambda: defaultdict(dict))\n",
    "for run_group, all_metrics_of_run_group in metrics_all.items():\n",
    "    print(run_group)\n",
    "    for run_abbrv, all_metrics_of_run_abbrv in all_metrics_of_run_group.items():\n",
    "        print(run_abbrv)\n",
    "        for run_name, metrics in all_metrics_of_run_abbrv.items():\n",
    "            print(run_name)\n",
    "            systems = metrics.pop(\"system\")\n",
    "            metrics_unrolled = {k: list(v.values()) for k, v in metrics.items()}\n",
    "            print(metrics_unrolled.keys())\n",
    "            unrolled_metrics_all_groups[run_group][run_abbrv][run_name] = (\n",
    "                metrics_unrolled\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_combined = {\n",
    "    **unrolled_metrics_all_groups[\"chattn\"],\n",
    "    **unrolled_metrics_all_groups[\"no_chattn\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_groups[\"no_chattn\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_metrics_dict(unrolled_metrics, metric_name):\n",
    "    summary_metrics_dict = defaultdict(dict)\n",
    "    for model_name, metrics_dict in unrolled_metrics.items():\n",
    "        print(model_name)\n",
    "        prediction_lengths = list(metrics_dict.keys())\n",
    "        summary_metrics_dict[model_name][\"prediction_lengths\"] = prediction_lengths\n",
    "        means = []\n",
    "        medians = []\n",
    "        stds = []\n",
    "        for prediction_length in prediction_lengths:\n",
    "            metric_val = metrics_dict[prediction_length][metric_name]\n",
    "            means.append(np.nanmean(metric_val))\n",
    "            medians.append(np.nanmedian(metric_val))\n",
    "            stds.append(np.nanstd(metric_val))\n",
    "        summary_metrics_dict[model_name][\"means\"] = means\n",
    "        summary_metrics_dict[model_name][\"medians\"] = medians\n",
    "        summary_metrics_dict[model_name][\"stds\"] = stds\n",
    "    return summary_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_prediction_length(\n",
    "    metrics_dict, metric_name, show_std_envelope=False\n",
    "):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    for model_name, metrics in metrics_dict.items():\n",
    "        plt.plot(\n",
    "            metrics[\"prediction_lengths\"],\n",
    "            metrics[\"medians\"],\n",
    "            marker=\"o\",\n",
    "            label=model_name,\n",
    "        )\n",
    "        std_envelope = np.array(metrics[\"stds\"])\n",
    "        if show_std_envelope:\n",
    "            plt.fill_between(\n",
    "                metrics[\"prediction_lengths\"],\n",
    "                metrics[\"means\"] - std_envelope,\n",
    "                metrics[\"means\"] + std_envelope,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Prediction Length\")\n",
    "    plt.title(metric_name, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dirs_all_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names_chosen = [\n",
    "    \"mse\",\n",
    "    \"mae\",\n",
    "    \"smape\",\n",
    "    \"r2_score\",\n",
    "    \"spearman\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict = defaultdict(dict)\n",
    "\n",
    "for run_group in run_metrics_dirs_all_groups.keys():\n",
    "    all_metrics_dict[run_group] = {\n",
    "        metrics_name: get_summary_metrics_dict(\n",
    "            unrolled_metrics_all_groups[run_group], metrics_name\n",
    "        )\n",
    "        for metrics_name in metric_names_chosen\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = plt.cm.tab10.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict[\"chattn\"][\"mse\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict: dict[str, dict[str, dict[str, list[float]]]],\n",
    "    metric_names: list[str],\n",
    "    runs_to_exclude: list[str],\n",
    "    metrics_to_show_ste_envelope: list[str] | None = None,\n",
    "    n_rows: int = 1,\n",
    "    n_cols: int = 3,\n",
    "    limit_num_prediction_lengths: int | None = None,\n",
    "    title: str | None = None,\n",
    "    custom_colors_dict: dict[str, str] | None = None,\n",
    "    show_legend: bool = True,\n",
    "    legend_kwargs: dict[str, Any] = {},\n",
    "    alpha_val: float = 0.8,\n",
    "    use_inv_spearman: bool = False,\n",
    "    legend_height_ratio: float = 0.2,  # New parameter to control legend height\n",
    "    subplot_size: tuple[float, float] = (4, 4),\n",
    "):\n",
    "    num_metrics = len(metric_names)\n",
    "\n",
    "    if custom_colors_dict is not None:\n",
    "        run_names_order = list(custom_colors_dict.keys())\n",
    "        # reorder all_metrics_dict keys according to run_names_order\n",
    "        for metric_name in all_metrics_dict.keys():\n",
    "            all_metrics_dict[metric_name] = {\n",
    "                k: all_metrics_dict[metric_name][k] for k in run_names_order\n",
    "            }\n",
    "\n",
    "    # Create figure with gridspec for better control over subplot heights\n",
    "    if show_legend:\n",
    "        fig = plt.figure(\n",
    "            figsize=(\n",
    "                subplot_size[0] * n_cols,\n",
    "                subplot_size[1] * n_rows * (1 + legend_height_ratio),\n",
    "            )\n",
    "        )\n",
    "        gs = plt.GridSpec(\n",
    "            n_rows + 1, n_cols, height_ratios=[1] * n_rows + [legend_height_ratio]\n",
    "        )\n",
    "        axes = [\n",
    "            fig.add_subplot(gs[i // n_cols, i % n_cols]) for i in range(num_metrics)\n",
    "        ]\n",
    "    else:\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows=n_rows,\n",
    "            ncols=n_cols,\n",
    "            figsize=(subplot_size[0] * n_cols, subplot_size[1] * n_rows),\n",
    "        )\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for i, (ax, metric_name) in enumerate(zip(axes, metric_names)):\n",
    "        metrics_dict = all_metrics_dict[metric_name]\n",
    "        for j, (model_name, metrics) in enumerate(metrics_dict.items()):\n",
    "            if model_name in runs_to_exclude:\n",
    "                continue\n",
    "            prediction_lengths = metrics[\"prediction_lengths\"][\n",
    "                :limit_num_prediction_lengths\n",
    "            ]\n",
    "            medians = metrics[\"medians\"][:limit_num_prediction_lengths]\n",
    "            means = metrics[\"means\"][:limit_num_prediction_lengths]\n",
    "            stds = metrics[\"stds\"][:limit_num_prediction_lengths]\n",
    "\n",
    "            if metric_name == \"spearman\" and use_inv_spearman:\n",
    "                medians = [1 - x for x in medians]\n",
    "                means = [1 - x for x in means]\n",
    "\n",
    "            chosen_color = (\n",
    "                custom_colors_dict[model_name]\n",
    "                if custom_colors_dict\n",
    "                else default_colors[j]\n",
    "            )\n",
    "            ax.plot(\n",
    "                prediction_lengths,\n",
    "                medians,\n",
    "                marker=\"o\",\n",
    "                label=model_name,\n",
    "                color=chosen_color,\n",
    "                alpha=alpha_val,\n",
    "            )\n",
    "            if (\n",
    "                metrics_to_show_ste_envelope is not None\n",
    "                and metric_name in metrics_to_show_ste_envelope\n",
    "            ):\n",
    "                # make ste (standard error) envelope\n",
    "                ste_envelope = np.array(stds) / np.sqrt(len(stds))\n",
    "                ax.fill_between(\n",
    "                    prediction_lengths,\n",
    "                    means - ste_envelope,\n",
    "                    means + ste_envelope,\n",
    "                    alpha=0.1,\n",
    "                    color=chosen_color,\n",
    "                )\n",
    "        ax.set_xlabel(\"Prediction Length\")\n",
    "        ax.set_xticks(prediction_lengths)\n",
    "        name = metric_name.replace(\"_\", \" \")\n",
    "        if name in [\"mse\", \"mae\", \"rmse\", \"mape\", \"smape\"]:\n",
    "            name = name.upper()\n",
    "        elif name == \"spearman\":\n",
    "            name = \"1 - Spearman\"\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        ax.set_title(name, fontweight=\"bold\")\n",
    "\n",
    "    if show_legend:\n",
    "        # Create legend subplot using gridspec\n",
    "        legend_ax = fig.add_subplot(gs[n_rows, :])\n",
    "        handles, labels = axes[0].get_legend_handles_labels()\n",
    "        legend_ax.legend(handles, labels, **legend_kwargs)\n",
    "        legend_ax.axis(\"off\")\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for ax in axes[num_metrics:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.suptitle(title, fontweight=\"bold\", fontsize=16, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_all_metrics_by_prediction_length(\n",
    "#     all_metrics_dict[\"chattn\"],\n",
    "#     [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "#     # metrics_to_show_ste_envelope=[\"smape\", \"pearson\", \"spearman\"],\n",
    "#     runs_to_exclude=[],\n",
    "#     metrics_to_show_ste_envelope=[],\n",
    "#     limit_num_prediction_lengths=None,\n",
    "#     title=\"Channel Attention\",\n",
    "#     n_cols=4,\n",
    "#     show_legend=True,\n",
    "#     legend_kwargs={\n",
    "#         \"loc\": \"center\",\n",
    "#         \"frameon\": True,\n",
    "#         \"ncol\": 4,\n",
    "#         \"framealpha\": 1.0,\n",
    "#         \"fontsize\": 16,\n",
    "#     },\n",
    "#     legend_height_ratio=0.4,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict_all = {\n",
    "    metrics_name: {\n",
    "        **all_metrics_dict[\"chattn\"][metrics_name],\n",
    "        **all_metrics_dict[\"no_chattn\"][metrics_name],\n",
    "    }\n",
    "    for metrics_name in metric_names_chosen\n",
    "}\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict_all,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    # metrics_to_show_ste_envelope=[\"smape\", \"pearson\", \"spearman\"],\n",
    "    runs_to_exclude=[],\n",
    "    metrics_to_show_ste_envelope=[],\n",
    "    limit_num_prediction_lengths=None,\n",
    "    title=\"All Models\",\n",
    "    n_cols=4,\n",
    "    show_legend=True,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"center\",\n",
    "        \"frameon\": True,\n",
    "        \"ncol\": 3,\n",
    "        \"framealpha\": 1.0,\n",
    "        \"fontsize\": 16,\n",
    "    },\n",
    "    legend_height_ratio=0.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def make_aggregate_plot(\n",
    "    unrolled_metrics: dict[str, dict[int, dict[str, list[float]]]],\n",
    "    prediction_length: int,\n",
    "    selected_run_names: list[str] | None = None,\n",
    "    ylim: tuple[float, float] = (1e-5, 1e5),\n",
    "    verbose: bool = False,\n",
    "    metrics_to_include: list[str] = [],\n",
    "    run_names_to_exclude: list[str] = [],\n",
    "    use_rescaled_smape: bool = False,\n",
    "    use_inv_spearman: bool = False,\n",
    "    title: str = \"Metrics\",\n",
    "    fig_kwargs: dict[str, Any] = {},\n",
    "    legend_kwargs: dict[str, Any] = {},\n",
    "    title_kwargs: dict[str, Any] = {},\n",
    "    bar_kwargs: dict[str, Any] = {},\n",
    "    plot_type: str = \"box\",  # New parameter: 'box' or 'bar'\n",
    "    colors: list[str] | None = None,\n",
    "    sort_by_metric: str | None = None,\n",
    "    save_path: str | None = None,\n",
    "):\n",
    "    if colors is None:\n",
    "        colors = plt.cm.tab10.colors\n",
    "    # Extract metrics data for the given prediction_length and run_names\n",
    "    if selected_run_names is None:\n",
    "        selected_run_names = list(unrolled_metrics.keys())\n",
    "\n",
    "    metrics_by_run_name = {\n",
    "        run_name: unrolled_metrics[run_name][prediction_length]\n",
    "        for run_name in selected_run_names\n",
    "    }\n",
    "\n",
    "    run_names = list(metrics_by_run_name.keys())\n",
    "    metric_names = list(metrics_by_run_name[run_names[0]].keys())\n",
    "    metric_names = [name for name in metric_names if name in metrics_to_include]\n",
    "    run_names = [name for name in run_names if name not in run_names_to_exclude]\n",
    "\n",
    "    print(f\"run_names: {run_names}\")\n",
    "    print(f\"metric_names: {metric_names}\")\n",
    "\n",
    "    # Create pretty titles for x-axis tick labels\n",
    "    metric_names_title = []\n",
    "    for name in metric_names:\n",
    "        # Create pretty titles for x-axis tick labels\n",
    "        if name in [\"mse\", \"mae\", \"rmse\", \"mape\"]:\n",
    "            name = name.upper()\n",
    "        elif name == \"smape\":\n",
    "            name = \"sMAPE\"\n",
    "        elif name == \"spearman\":\n",
    "            name = \"1 - Spearman\"\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        metric_names_title.append(name)\n",
    "\n",
    "    if fig_kwargs == {}:\n",
    "        fig_kwargs = {\"figsize\": (6, 4)}\n",
    "    plt.figure(**fig_kwargs)\n",
    "\n",
    "    plot_data = []\n",
    "    median_data = []\n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        metric_title = metric_names_title[i]\n",
    "        for run_name in run_names:\n",
    "            values = metrics_by_run_name[run_name][metric_name]\n",
    "            if metric_name == \"smape\" and use_rescaled_smape:\n",
    "                values = [x / 200 for x in values]\n",
    "            if metric_name == \"spearman\" and use_inv_spearman:\n",
    "                values = [1 - x for x in values]\n",
    "            median_value = np.nanmedian(values)\n",
    "            plot_data.extend([(metric_title, v, run_name) for v in values])\n",
    "            median_data.append((metric_title, median_value, run_name))\n",
    "            if verbose:\n",
    "                print(f\"{metric_title} - {run_name} median: {median_value}\")\n",
    "\n",
    "    # Create DataFrame for use with seaborn\n",
    "    df = pd.DataFrame(plot_data, columns=[\"Metric\", \"Value\", \"Run\"])\n",
    "\n",
    "    # Set the order of metrics to match the original order in metric_names_title\n",
    "    df[\"Metric\"] = pd.Categorical(\n",
    "        df[\"Metric\"], categories=metric_names_title, ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort runs based on the specified metric or overall median\n",
    "    if sort_by_metric is not None:\n",
    "        # Find the pretty title for the specified metric\n",
    "        sort_metric_title = None\n",
    "        for i, metric_name in enumerate(metric_names):\n",
    "            if metric_name.lower() == sort_by_metric.lower():\n",
    "                sort_metric_title = metric_names_title[i]\n",
    "                break\n",
    "\n",
    "        if sort_metric_title is None:\n",
    "            print(\n",
    "                f\"Warning: Metric '{sort_by_metric}' not found. Using overall median for sorting.\"\n",
    "            )\n",
    "            # Calculate the median for each run (across all metrics)\n",
    "            median_values = df.groupby(\"Run\")[\"Value\"].median().reset_index()\n",
    "        else:\n",
    "            # Filter data for the specific metric and calculate median\n",
    "            metric_df = df[df[\"Metric\"] == sort_metric_title]\n",
    "            median_values = metric_df.groupby(\"Run\")[\"Value\"].median().reset_index()\n",
    "    else:\n",
    "        # Calculate the median for each run (across all metrics)\n",
    "        median_values = df.groupby(\"Run\")[\"Value\"].median().reset_index()\n",
    "\n",
    "    # Sort the medians\n",
    "    sorted_medians = median_values.sort_values(\"Value\")\n",
    "\n",
    "    # Create a categorical type with the sorted run names to preserve the order\n",
    "    df[\"Run\"] = pd.Categorical(\n",
    "        df[\"Run\"], categories=sorted_medians[\"Run\"].tolist(), ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort the original dataframe\n",
    "    df = df.sort_values(\"Run\")\n",
    "\n",
    "    # Choose plot type based on parameter\n",
    "    if plot_type == \"box\":\n",
    "        # Plot box plot\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Run\",\n",
    "            dodge=True,\n",
    "            width=0.8,\n",
    "            fliersize=0,  # Don't show outlier points\n",
    "            palette=colors[: len(run_names)],\n",
    "            medianprops={\"linewidth\": 2.5, \"solid_capstyle\": \"butt\"},\n",
    "            saturation=0.6,\n",
    "        )\n",
    "    elif plot_type == \"bar\":\n",
    "        if bar_kwargs == {}:\n",
    "            bar_kwargs = {\"estimator\": \"median\"}\n",
    "        # Plot bar plot\n",
    "        sns.barplot(\n",
    "            data=df,\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Run\",\n",
    "            palette=colors[: len(run_names)],\n",
    "            saturation=0.6,\n",
    "            **bar_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported plot_type: {plot_type}\")\n",
    "\n",
    "    plt.ylim(ylim)\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.xticks(fontweight=\"bold\")\n",
    "    plt.legend(**legend_kwargs)\n",
    "    plt.title(title, fontweight=\"bold\", **title_kwargs)\n",
    "    plt.xticks(rotation=15)  # Optional: rotates x-tick labels for readability\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_groups[\"chattn\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_combined.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_gray = (0.7, 0.7, 0.7)\n",
    "bar_colors = list(plt.cm.tab20c.colors[:8]) + [light_gray]\n",
    "print(len(bar_colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot(\n",
    "    unrolled_metrics_all_combined,\n",
    "    128,\n",
    "    selected_run_names=None,\n",
    "    ylim=(0, 1),\n",
    "    metrics_to_include=[\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    use_rescaled_smape=True,\n",
    "    use_inv_spearman=True,\n",
    "    title=\"Zeroshot Metrics for Ablations\",\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper right\",\n",
    "        \"fontsize\": 6,\n",
    "        \"frameon\": True,\n",
    "        \"framealpha\": 1.0,\n",
    "    },\n",
    "    plot_type=\"box\",\n",
    "    colors=bar_colors,\n",
    "    sort_by_metric=\"smape\",\n",
    "    save_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_aggregate_plot_v2(\n",
    "    unrolled_metrics: dict[str, dict[int, dict[str, list[float]]]],\n",
    "    prediction_length: int,\n",
    "    metric_to_plot: str = \"smape\",  # Default to smape\n",
    "    selected_run_names: list[str] | None = None,\n",
    "    ylim: tuple[float, float] | None = None,  # Changed to None as default\n",
    "    verbose: bool = False,\n",
    "    run_names_to_exclude: list[str] = [],\n",
    "    use_rescaled_smape: bool = False,\n",
    "    use_inv_spearman: bool = False,\n",
    "    title: str | None = None,\n",
    "    fig_kwargs: dict[str, Any] = {},\n",
    "    title_kwargs: dict[str, Any] = {},\n",
    "    colors: list[str] | None = None,\n",
    "    sort_runs: bool = False,\n",
    "    save_path: str | None = None,\n",
    "    order_by_metric: str | None = None,\n",
    "    ylabel_fontsize: int = 8,\n",
    "    show_xlabel: bool = True,\n",
    "    show_legend: bool = False,\n",
    "    legend_kwargs: dict[str, Any] = {},\n",
    "    alpha_val: float = 0.8,\n",
    "    box_percentile_range: tuple[int, int] = (25, 75),\n",
    "    whisker_percentile_range: tuple[float, float] = (0, 90),\n",
    ") -> list[mpatches.Patch] | None:\n",
    "    # Set default figure size if not provided\n",
    "    if fig_kwargs == {}:\n",
    "        fig_kwargs = {\"figsize\": (3, 5)}  # Wider figure to accommodate run names\n",
    "\n",
    "    if colors is None:\n",
    "        colors = plt.cm.tab10.colors  # type: ignore\n",
    "\n",
    "    # Extract metrics data for the given prediction_length and run_names\n",
    "    if selected_run_names is None:\n",
    "        selected_run_names = list(unrolled_metrics.keys())\n",
    "\n",
    "    # Filter out excluded run names\n",
    "    run_names = [\n",
    "        name for name in selected_run_names if name not in run_names_to_exclude\n",
    "    ]\n",
    "\n",
    "    if len(run_names) == 0:\n",
    "        print(\"No run names to plot after exclusions!\")\n",
    "        return\n",
    "\n",
    "    plt.figure(**fig_kwargs)\n",
    "    # Collect data for plotting\n",
    "    plot_data = []\n",
    "\n",
    "    # Add a dictionary to store ordering metric values if needed\n",
    "    ordering_metric_data = {}\n",
    "\n",
    "    for run_name in run_names:\n",
    "        try:\n",
    "            # Check if this run has data for the specified prediction length\n",
    "            if prediction_length not in unrolled_metrics[run_name]:\n",
    "                print(\n",
    "                    f\"Warning: prediction_length {prediction_length} not found for {run_name}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Check if this run has the specified metric\n",
    "            if metric_to_plot not in unrolled_metrics[run_name][prediction_length]:\n",
    "                print(f\"Warning: metric '{metric_to_plot}' not found for {run_name}\")\n",
    "                continue\n",
    "\n",
    "            values = unrolled_metrics[run_name][prediction_length][metric_to_plot]\n",
    "\n",
    "            # Process values based on metric type\n",
    "            if metric_to_plot == \"smape\" and use_rescaled_smape:\n",
    "                values = [x / 200 for x in values]\n",
    "            if metric_to_plot == \"spearman\" and use_inv_spearman:\n",
    "                values = [1 - x for x in values]\n",
    "\n",
    "            # Filter out NaN values\n",
    "            values = [v for v in values if not np.isnan(v)]\n",
    "\n",
    "            if len(values) == 0:\n",
    "                print(f\"Warning: All values for {run_name} are NaN\")\n",
    "                continue\n",
    "\n",
    "            median_value = np.median(values)\n",
    "            plot_data.extend([(run_name, v) for v in values])\n",
    "\n",
    "            # If we need to order by a different metric, collect that data too\n",
    "            if order_by_metric is not None and order_by_metric != metric_to_plot:\n",
    "                if order_by_metric in unrolled_metrics[run_name][prediction_length]:\n",
    "                    order_values = unrolled_metrics[run_name][prediction_length][\n",
    "                        order_by_metric\n",
    "                    ]\n",
    "\n",
    "                    # Apply same processing as we would for the plotting metric\n",
    "                    if order_by_metric == \"smape\" and use_rescaled_smape:\n",
    "                        order_values = [x / 200 for x in order_values]\n",
    "                    if order_by_metric == \"spearman\" and use_inv_spearman:\n",
    "                        order_values = [1 - x for x in order_values]\n",
    "\n",
    "                    # Filter out NaN values\n",
    "                    order_values = [v for v in order_values if not np.isnan(v)]\n",
    "\n",
    "                    if order_values:\n",
    "                        ordering_metric_data[run_name] = np.median(order_values)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{run_name} median {metric_to_plot}: {median_value}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {run_name}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(plot_data, columns=[\"Run\", \"Value\"])\n",
    "\n",
    "    # Determine run order based on specified criteria\n",
    "    if order_by_metric is not None and ordering_metric_data:\n",
    "        # Sort runs based on the ordering metric's median values\n",
    "        run_order = [\n",
    "            run for run, _ in sorted(ordering_metric_data.items(), key=lambda x: x[1])\n",
    "        ]\n",
    "        # Only include runs that are in our dataframe\n",
    "        run_order = [run for run in run_order if run in df[\"Run\"].unique()]\n",
    "        # Set categorical order for Run column\n",
    "        df[\"Run\"] = pd.Categorical(df[\"Run\"], categories=run_order, ordered=True)\n",
    "    elif sort_runs:\n",
    "        # Use the existing sort_runs logic if order_by_metric isn't specified\n",
    "        median_by_run = df.groupby(\"Run\")[\"Value\"].median().sort_values()\n",
    "        run_order = median_by_run.index.tolist()\n",
    "        df[\"Run\"] = pd.Categorical(df[\"Run\"], categories=run_order, ordered=True)\n",
    "\n",
    "    metric_title = metric_to_plot\n",
    "    if metric_to_plot in [\"mse\", \"mae\", \"rmse\", \"mape\"]:\n",
    "        metric_title = metric_to_plot.upper()\n",
    "    elif metric_to_plot == \"smape\":\n",
    "        metric_title = \"sMAPE\"\n",
    "    elif metric_to_plot == \"spearman\":\n",
    "        metric_title = \"1 - Spearman\" if use_inv_spearman else \"Spearman\"\n",
    "    else:\n",
    "        metric_title = metric_to_plot.capitalize()\n",
    "\n",
    "    # Create a custom boxplot with the specified percentile ranges\n",
    "    ax = plt.gca()\n",
    "\n",
    "    unique_runs = (\n",
    "        df[\"Run\"].unique()\n",
    "        if not isinstance(df[\"Run\"].dtype, pd.CategoricalDtype)\n",
    "        else df[\"Run\"].cat.categories\n",
    "    )\n",
    "\n",
    "    for i, run in enumerate(unique_runs):\n",
    "        run_data = df[df[\"Run\"] == run][\"Value\"].to_numpy()\n",
    "        if len(run_data) == 0:\n",
    "            continue\n",
    "\n",
    "        # Calculate the percentiles\n",
    "        lower_box, upper_box = np.percentile(run_data, box_percentile_range)\n",
    "        lower_whisker, upper_whisker = np.percentile(run_data, whisker_percentile_range)\n",
    "        median_val = np.median(run_data)\n",
    "\n",
    "        # Plot the custom boxplot\n",
    "        color = colors[i % len(colors)]  # type: ignore\n",
    "\n",
    "        # Box\n",
    "        box = plt.Rectangle(\n",
    "            (i - 0.3, lower_box),\n",
    "            0.6,\n",
    "            upper_box - lower_box,\n",
    "            fill=True,\n",
    "            facecolor=color,\n",
    "            alpha=alpha_val,\n",
    "            linewidth=1,\n",
    "            edgecolor=\"black\",\n",
    "            zorder=5,\n",
    "        )\n",
    "        ax.add_patch(box)\n",
    "\n",
    "        # Median line\n",
    "        ax.hlines(\n",
    "            median_val, i - 0.3, i + 0.3, colors=\"black\", linewidth=2.5, zorder=10\n",
    "        )\n",
    "\n",
    "        # Whiskers\n",
    "        ax.vlines(\n",
    "            i,\n",
    "            lower_box,\n",
    "            lower_whisker,\n",
    "            colors=\"black\",\n",
    "            linestyle=\"-\",\n",
    "            linewidth=1,\n",
    "            zorder=5,\n",
    "        )\n",
    "        ax.vlines(\n",
    "            i,\n",
    "            upper_box,\n",
    "            upper_whisker,\n",
    "            colors=\"black\",\n",
    "            linestyle=\"-\",\n",
    "            linewidth=1,\n",
    "            zorder=5,\n",
    "        )\n",
    "\n",
    "        # Caps on whiskers\n",
    "        ax.hlines(\n",
    "            lower_whisker, i - 0.15, i + 0.15, colors=\"black\", linewidth=1, zorder=5\n",
    "        )\n",
    "        ax.hlines(\n",
    "            upper_whisker, i - 0.15, i + 0.15, colors=\"black\", linewidth=1, zorder=5\n",
    "        )\n",
    "\n",
    "    # Set y-limits if provided\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "\n",
    "    # Format the plot\n",
    "    plt.ylabel(metric_title, fontweight=\"bold\", fontsize=ylabel_fontsize)\n",
    "    plt.xlabel(\"\")\n",
    "    if show_xlabel:\n",
    "        # Format x-axis labels\n",
    "        plt.xticks(\n",
    "            range(len(unique_runs)),\n",
    "            unique_runs,\n",
    "            rotation=45,\n",
    "            ha=\"right\",\n",
    "            fontsize=5,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "    else:\n",
    "        plt.xticks([])\n",
    "\n",
    "    # Set the title\n",
    "    if title is not None:\n",
    "        title_with_metric = f\"{title}: {metric_title}\" if title == \"Metrics\" else title\n",
    "        plt.title(title_with_metric, fontweight=\"bold\", **title_kwargs)\n",
    "\n",
    "    # Ensure plot is properly displayed\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Get the unique run names in the correct order\n",
    "    if isinstance(df[\"Run\"].dtype, pd.CategoricalDtype):\n",
    "        runs = df[\"Run\"].cat.categories.tolist()\n",
    "    else:\n",
    "        runs = df[\"Run\"].unique().tolist()\n",
    "\n",
    "    # Create custom legend handles\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color=colors[i % len(colors)], label=run, alpha=alpha_val)  # type: ignore\n",
    "        for i, run in enumerate(runs)\n",
    "    ]\n",
    "\n",
    "    if show_legend:\n",
    "        plt.legend(handles=legend_handles, **legend_kwargs)\n",
    "\n",
    "    # Save if path provided\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "\n",
    "    plt.show()\n",
    "    return legend_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pred_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_aggregate_plot_v2(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=selected_pred_length,\n",
    "    metric_to_plot=\"smape\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    save_path=\"ablations_figs/smape_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(40, 60),\n",
    "    whisker_percentile_range=(25, 75),\n",
    "    alpha_val=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot_v2(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=selected_pred_length,\n",
    "    metric_to_plot=\"spearman\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=\"ablations_figs/spearman_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(40, 60),\n",
    "    whisker_percentile_range=(25, 75),\n",
    "    alpha_val=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot_v2(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=selected_pred_length,\n",
    "    metric_to_plot=\"mae\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=\"ablations_figs/mae_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(40, 60),\n",
    "    whisker_percentile_range=(25, 75),\n",
    "    alpha_val=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = make_aggregate_plot_v2(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=selected_pred_length,\n",
    "    metric_to_plot=\"mse\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=\"ablations_figs/mse_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper left\",\n",
    "        \"frameon\": True,\n",
    "        \"ncol\": 1,\n",
    "        \"framealpha\": 0.8,\n",
    "        # \"prop\": {\"weight\": \"bold\", \"size\": 5},\n",
    "        \"prop\": {\"size\": 6.8},\n",
    "    },\n",
    "    box_percentile_range=(40, 60),\n",
    "    whisker_percentile_range=(25, 75),\n",
    "    alpha_val=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 0.6))\n",
    "# Add the legend\n",
    "plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"center\",\n",
    "    frameon=True,\n",
    "    ncol=3,\n",
    "    framealpha=1.0,\n",
    ")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"ablations_figs/ablations_legend.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles[0].get_label(), legend_handles[0].get_facecolor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colors_dict = {}\n",
    "for i, patch in enumerate(legend_handles):\n",
    "    color = patch.get_facecolor()\n",
    "    hex_color = mcolors.rgb2hex(color)\n",
    "    run_name = patch.get_label()\n",
    "    print(run_name, hex_color)\n",
    "    custom_colors_dict[run_name] = hex_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict_all[\"smape\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict_all,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    # metrics_to_show_ste_envelope=[\"smape\", \"spearman\"],\n",
    "    runs_to_exclude=[],\n",
    "    limit_num_prediction_lengths=None,\n",
    "    title=None,\n",
    "    n_rows=1,\n",
    "    n_cols=4,\n",
    "    custom_colors_dict=custom_colors_dict,\n",
    "    show_legend=True,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"center\",\n",
    "        \"frameon\": True,\n",
    "        \"ncol\": 3,\n",
    "        \"framealpha\": 1.0,\n",
    "        \"fontsize\": 16,\n",
    "    },\n",
    "    legend_height_ratio=0.4,\n",
    "    alpha_val=0.8,\n",
    "    use_inv_spearman=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_run_names = [\n",
    "#     \"chattn + mlm + embedding\",\n",
    "#     \"chattn + mlm\",\n",
    "#     \"chattn + embedding\",\n",
    "#     \"chattn\",\n",
    "#     \"univariate (wider) + embedding\",\n",
    "#     \"univariate (wider)\",\n",
    "#     \"univariate (deeper)\",\n",
    "#     \"univariate + mlm + embedding\",\n",
    "#     \"univariate + mlm\",\n",
    "# ]\n",
    "\n",
    "run_names_to_exclude = [\n",
    "    # \"chattn + mlm + embedding\",\n",
    "    # \"chattn + mlm\",\n",
    "    # \"chattn + embedding\",\n",
    "    # \"chattn\",\n",
    "    \"univariate (wider) + embedding\",\n",
    "    \"univariate (wider)\",\n",
    "    \"univariate (deeper)\",\n",
    "    \"univariate + mlm + embedding\",\n",
    "    \"univariate + mlm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict_all,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    # metrics_to_show_ste_envelope=[\"smape\", \"spearman\"],\n",
    "    runs_to_exclude=run_names_to_exclude,\n",
    "    limit_num_prediction_lengths=None,\n",
    "    title=None,\n",
    "    n_rows=1,\n",
    "    n_cols=4,\n",
    "    # custom_colors_dict=custom_colors_dict,\n",
    "    show_legend=True,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"center\",\n",
    "        \"frameon\": True,\n",
    "        \"ncol\": 4,\n",
    "        \"framealpha\": 1.0,\n",
    "        \"fontsize\": 16,\n",
    "    },\n",
    "    legend_height_ratio=0.4,\n",
    "    alpha_val=0.8,\n",
    "    use_inv_spearman=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_names_to_exclude = [\n",
    "    # \"chattn + mlm + embedding\",\n",
    "    # \"chattn + mlm\",\n",
    "    # \"chattn + embedding\",\n",
    "    # \"chattn\",\n",
    "    \"univariate (wider) + embedding\",\n",
    "    \"univariate (wider)\",\n",
    "    \"univariate (deeper)\",\n",
    "    \"univariate + mlm + embedding\",\n",
    "    \"univariate + mlm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict_all,\n",
    "    [\"smape\", \"spearman\"],\n",
    "    metrics_to_show_ste_envelope=[\"smape\", \"spearman\"],\n",
    "    runs_to_exclude=run_names_to_exclude,\n",
    "    limit_num_prediction_lengths=None,\n",
    "    title=None,\n",
    "    n_rows=1,\n",
    "    n_cols=2,\n",
    "    custom_colors_dict=custom_colors_dict,\n",
    "    show_legend=True,\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"center\",\n",
    "        \"frameon\": True,\n",
    "        \"ncol\": 2,\n",
    "        \"framealpha\": 1.0,\n",
    "        \"fontsize\": 16,\n",
    "    },\n",
    "    legend_height_ratio=0.4,\n",
    "    alpha_val=0.8,\n",
    "    use_inv_spearman=True,\n",
    "    subplot_size=(4, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_names_to_exclude = [\n",
    "    # \"chattn + mlm + embedding\",\n",
    "    # \"chattn + mlm\",\n",
    "    \"chattn + embedding\",\n",
    "    \"chattn\",\n",
    "    \"univariate (wider) + embedding\",\n",
    "    \"univariate (wider)\",\n",
    "    \"univariate (deeper)\",\n",
    "    \"univariate + mlm + embedding\",\n",
    "    \"univariate + mlm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict = all_metrics_dict_all[\"smape\"]\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "colors = plt.cm.tab20c.colors\n",
    "markers = [\"o\", \"s\", \"D\", \"P\"]\n",
    "linestyles = [\"-\", \"--\", \"-.\", \":\"]\n",
    "for j, (model_name, metrics) in enumerate(smape_metrics_dict.items()):\n",
    "    if model_name in run_names_to_exclude:\n",
    "        continue\n",
    "    prediction_lengths = metrics[\"prediction_lengths\"]\n",
    "    medians = metrics[\"medians\"]\n",
    "    means = metrics[\"means\"]\n",
    "    stds = metrics[\"stds\"]\n",
    "\n",
    "    plt.plot(\n",
    "        prediction_lengths,\n",
    "        medians,\n",
    "        marker=markers[j],\n",
    "        label=model_name,\n",
    "        color=colors[j],\n",
    "        alpha=1,\n",
    "        linestyle=linestyles[j],\n",
    "    )\n",
    "    # make ste (standard error) envelope\n",
    "    ste_envelope = np.array(stds) / np.sqrt(len(stds))\n",
    "    plt.fill_between(\n",
    "        prediction_lengths,\n",
    "        means - ste_envelope,\n",
    "        means + ste_envelope,\n",
    "        alpha=0.1,\n",
    "        color=colors[j],\n",
    "    )\n",
    "plt.xlabel(\"Prediction Length\", fontweight=\"bold\")\n",
    "plt.xticks(prediction_lengths)\n",
    "plt.title(\"sMAPE\", fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", frameon=True, framealpha=1.0, fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ablations_figs/smape_embedding_ablation.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
