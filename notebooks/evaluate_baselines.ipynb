{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dystformer.utils import (\n",
    "    apply_custom_style,\n",
    "    make_box_plot,\n",
    "    plot_all_metrics_by_prediction_length,\n",
    ")\n",
    "\n",
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_COLORS = list(plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"])\n",
    "figs_save_dir = os.path.join(\"../figures\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"test_zeroshot\"\n",
    "\n",
    "run_metrics_dir_dict = {\n",
    "    \"Panda\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"patchtst\",\n",
    "        # \"pft_stand_rff_only_pretrained-0\",\n",
    "        # \"pft_chattn_noembed_pretrained_correct-0\",\n",
    "        \"pft_chattn_emb_w_poly-0\",\n",
    "        # \"pft_linattnpolyemb_from_scratch-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M SFT\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        # \"chronos_bolt_mini-12\",\n",
    "        \"chronos_mini_ft-0\",\n",
    "        # \"chronos_finetune_stand_updated-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        \"chronos_mini_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Time MOE 50M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"timemoe\",\n",
    "        \"timemoe-50m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"TimesFM 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"timesfm\",\n",
    "        \"timesfm-200m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    # \"Mean\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"mean\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Fourier\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"fourier\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs = defaultdict(dict)\n",
    "for model_name, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "    if not os.path.exists(run_metrics_dir):\n",
    "        print(f\"Run metrics dir does not exist: {run_metrics_dir}\")\n",
    "        continue\n",
    "    for file in sorted(\n",
    "        os.listdir(run_metrics_dir),\n",
    "        key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "    ):\n",
    "        if file.endswith(\".csv\"):\n",
    "            prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "            with open(os.path.join(run_metrics_dir, file), \"r\") as f:\n",
    "                metrics = pd.read_csv(f).to_dict()\n",
    "                metrics_all_runs[model_name][prediction_length] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics = defaultdict(dict)\n",
    "for model_name, all_metrics_of_model in metrics_all_runs.items():\n",
    "    for prediction_length, metrics in all_metrics_of_model.items():\n",
    "        systems = metrics[\"system\"]\n",
    "        metrics_unrolled = {\n",
    "            k: list(v.values()) for k, v in metrics.items() if k != \"system\"\n",
    "        }\n",
    "        unrolled_metrics[model_name][prediction_length] = metrics_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = len(unrolled_metrics.keys())\n",
    "print(n_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = DEFAULT_COLORS[: n_runs + 1]\n",
    "default_colors = default_colors[:3] + default_colors[4:]\n",
    "print(default_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"smape\"\n",
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"{figs_save_dir}/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(25, 75),\n",
    "    whisker_percentile_range=(5, 95),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2, 4)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=5,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\n",
    "    f\"{figs_save_dir}/baselines_legend_horizontal_patches.pdf\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_metrics_dict(\n",
    "    unrolled_metrics: dict, metric_name: str\n",
    ") -> tuple[dict[str, dict[str, list[float]]], dict[str, bool]]:\n",
    "    \"\"\"\n",
    "    Get the summary metrics for a given metric name.\n",
    "\n",
    "    Returns a tuple of the summary metrics dictionary and a boolean indicating whether there are NaNs in the metrics.\n",
    "    \"\"\"\n",
    "    summary_metrics_dict = defaultdict(dict)\n",
    "    has_nans = defaultdict(bool)\n",
    "    for model_name, metrics_dict in unrolled_metrics.items():\n",
    "        prediction_lengths = list(metrics_dict.keys())\n",
    "        summary_metrics_dict[model_name][\"prediction_lengths\"] = prediction_lengths\n",
    "        num_vals = len(metrics_dict[prediction_lengths[0]][metric_name])\n",
    "        summary_metrics_dict[model_name][\"num_vals\"] = num_vals\n",
    "        means = []\n",
    "        medians = []\n",
    "        stds = []\n",
    "        stes = []\n",
    "        all_vals = []\n",
    "        for prediction_length in tqdm(\n",
    "            prediction_lengths, desc=f\"Computing {metric_name} for {model_name}\"\n",
    "        ):\n",
    "            metric_vals = np.array(metrics_dict[prediction_length][metric_name])\n",
    "            if np.isnan(metric_vals).any():\n",
    "                has_nans[model_name] = True\n",
    "            mean = np.nanmean(metric_vals)\n",
    "            median = np.nanmedian(metric_vals)\n",
    "            std = np.nanstd(metric_vals)\n",
    "            ste = std / np.sqrt(len(metric_vals))\n",
    "\n",
    "            means.append(mean)\n",
    "            medians.append(median)\n",
    "            stds.append(std)\n",
    "            stes.append(ste)\n",
    "            all_vals.append(metric_vals)\n",
    "\n",
    "        if has_nans[model_name]:\n",
    "            print(f\"NaNs in {model_name} for {metric_name}\")\n",
    "\n",
    "        summary_metrics_dict[model_name][\"means\"] = means\n",
    "        summary_metrics_dict[model_name][\"medians\"] = medians\n",
    "        summary_metrics_dict[model_name][\"stds\"] = stds\n",
    "        summary_metrics_dict[model_name][\"stes\"] = stes\n",
    "        summary_metrics_dict[model_name][\"all_vals\"] = all_vals\n",
    "\n",
    "    return summary_metrics_dict, has_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict, has_nans = get_summary_metrics_dict(unrolled_metrics, \"smape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mse\", \"mae\", \"smape\"]\n",
    "metrics_dicts, has_nans = zip(\n",
    "    *[get_summary_metrics_dict(unrolled_metrics, metric) for metric in metrics]\n",
    ")\n",
    "all_metrics_dict = {m: metrics_dicts[i] for i, m in enumerate(metrics)}\n",
    "has_nans_dict = {m: has_nans[i] for i, m in enumerate(metrics)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_nans_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order model names by sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_ordering = []  # sorted by median smape at 128\n",
    "for model_name, data in all_metrics_dict[\"smape\"].items():\n",
    "    median_metrics_128 = data[\"medians\"][1]\n",
    "    model_names_ordering.append((model_name, median_metrics_128))\n",
    "model_names_ordering = sorted(model_names_ordering, key=lambda x: x[1])\n",
    "model_names_ordering = [x[0] for x in model_names_ordering]\n",
    "print(model_names_ordering)\n",
    "\n",
    "# Reorder all_metrics_dict according to model_names_ordering for each metric\n",
    "reordered_metrics_dict = {}\n",
    "for metric_name, metric_data in all_metrics_dict.items():\n",
    "    reordered_metric_data = {}\n",
    "\n",
    "    # Add models in the order specified by model_names_ordering\n",
    "    for model_name in model_names_ordering:\n",
    "        if model_name in metric_data:\n",
    "            reordered_metric_data[model_name] = metric_data[model_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not found in {metric_name}\")\n",
    "\n",
    "    reordered_metrics_dict[metric_name] = reordered_metric_data\n",
    "all_metrics_dict = reordered_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"mse\", \"mae\", \"smape\"],\n",
    "    metrics_to_show_envelope=[\"mae\", \"smape\"],\n",
    "    n_cols=4,\n",
    "    n_rows=1,\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_metrics_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"loc\": \"upper left\", \"frameon\": True, \"fontsize\": 10},\n",
    "    colors=default_colors,\n",
    "    use_inv_spearman=True,\n",
    "    percentile_range=(40, 60),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=5,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(f\"{figs_save_dir}/baselines_legend_horizontal.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"smape\"],\n",
    "    metrics_to_show_envelope=[\"smape\"],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    ylim=(10, None),\n",
    "    save_path=f\"{figs_save_dir}/zeroshot_smape_autoregressive_rollout_metrics.pdf\",\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(35, 65),\n",
    "    has_nans=has_nans_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer_jeff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
