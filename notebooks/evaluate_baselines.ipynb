{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dystformer.utils import apply_custom_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply matplotlib style from config\n",
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../figures\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"final_skew40/test_zeroshot\"\n",
    "\n",
    "run_metrics_dir_dict = {\n",
    "    \"Our Model\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"patchtst\",\n",
    "        # \"pft_stand_rff_only_pretrained-0\",\n",
    "        \"pft_chattn_noembed_pretrained_correct-0\",\n",
    "        # \"pft_chattn_emb_w_poly-0\"\n",
    "        # \"pft_chattn_noembed_pretrained_correct-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M Finetune\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        \"chronos_bolt_mini-12\",\n",
    "        # \"chronos_mini_ft-0\",\n",
    "        # \"chronos_finetune_stand_updated-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        \"chronos_mini_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Time MOE 50M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"timemoe\",\n",
    "        \"timemoe-50m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"TimesFM 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"timesfm\",\n",
    "        \"timesfm-200m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    # \"Mean\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"mean\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Fourier\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"fourier\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dir_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs = defaultdict(dict)\n",
    "for model_name, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "    print(model_name)\n",
    "    if not os.path.exists(run_metrics_dir):\n",
    "        print(f\"Run metrics dir does not exist: {run_metrics_dir}\")\n",
    "        continue\n",
    "    for file in sorted(\n",
    "        os.listdir(run_metrics_dir),\n",
    "        key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "    ):\n",
    "        if file.endswith(\".csv\"):\n",
    "            prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "            # print(prediction_length)\n",
    "            with open(os.path.join(run_metrics_dir, file), \"r\") as f:\n",
    "                metrics = pd.read_csv(f).to_dict()\n",
    "                metrics_all_runs[model_name][prediction_length] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs[\"Our Model\"][64].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics = defaultdict(dict)\n",
    "for model_name, all_metrics_of_model in metrics_all_runs.items():\n",
    "    print(model_name)\n",
    "    for prediction_length, metrics in all_metrics_of_model.items():\n",
    "        systems = metrics[\"system\"]\n",
    "        metrics_unrolled = {\n",
    "            k: list(v.values()) for k, v in metrics.items() if k != \"system\"\n",
    "        }\n",
    "        print(metrics_unrolled.keys())\n",
    "        unrolled_metrics[model_name][prediction_length] = metrics_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Our Model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Our Model\"][64].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unrolled_metrics[\"Our Model\"][64][\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Our Model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = len(unrolled_metrics.keys())\n",
    "print(n_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "default_colors = default_colors[: n_runs + 1]\n",
    "default_colors = default_colors[:3] + default_colors[4:]\n",
    "print(default_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"mse\"\n",
    "make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"baselines_figs/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(40, 60),\n",
    "    whisker_percentile_range=(25, 75),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2, 5)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_metrics_dict(\n",
    "    unrolled_metrics: dict, metric_name: str\n",
    ") -> dict[str, dict[str, list[float]]]:\n",
    "    summary_metrics_dict = defaultdict(dict)\n",
    "    for model_name, metrics_dict in unrolled_metrics.items():\n",
    "        print(model_name)\n",
    "        prediction_lengths = list(metrics_dict.keys())\n",
    "        summary_metrics_dict[model_name][\"prediction_lengths\"] = prediction_lengths\n",
    "        means = []\n",
    "        medians = []\n",
    "        stds = []\n",
    "        for prediction_length in prediction_lengths:\n",
    "            metric_val = metrics_dict[prediction_length][metric_name]\n",
    "            means.append(np.nanmean(metric_val))\n",
    "            medians.append(np.nanmedian(metric_val))\n",
    "            stds.append(np.nanstd(metric_val))\n",
    "        summary_metrics_dict[model_name][\"means\"] = means\n",
    "        summary_metrics_dict[model_name][\"medians\"] = medians\n",
    "        summary_metrics_dict[model_name][\"stds\"] = stds\n",
    "    return summary_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict = get_summary_metrics_dict(unrolled_metrics, \"smape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict[\"Ours\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_prediction_length(\n",
    "    metrics_dict, metric_name, show_std_envelope=False\n",
    "):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    for model_name, metrics in metrics_dict.items():\n",
    "        plt.plot(\n",
    "            metrics[\"prediction_lengths\"],\n",
    "            metrics[\"medians\"],\n",
    "            marker=\"o\",\n",
    "            label=model_name,\n",
    "        )\n",
    "        std_envelope = np.array(metrics[\"stds\"])\n",
    "        if show_std_envelope:\n",
    "            plt.fill_between(\n",
    "                metrics[\"prediction_lengths\"],\n",
    "                metrics[\"means\"] - std_envelope,\n",
    "                metrics[\"means\"] + std_envelope,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Prediction Length\")\n",
    "    plt.title(metric_name, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_metrics_by_prediction_length(smape_metrics_dict, \"sMAPE\", show_std_envelope=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict = {\n",
    "    metrics_name: get_summary_metrics_dict(unrolled_metrics, metrics_name)\n",
    "    for metrics_name in [\n",
    "        \"mse\",\n",
    "        \"mae\",\n",
    "        \"smape\",\n",
    "        \"spearman\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict[\"mse\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = [\"o\", \"s\", \"v\", \"D\", \"X\", \"P\", \"H\", \"h\", \"d\", \"p\", \"x\"]\n",
    "\n",
    "\n",
    "def plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict: dict[str, dict[str, dict[str, list[float]]]],\n",
    "    metric_names: list[str],\n",
    "    metrics_to_show_std_envelope: list[str],\n",
    "    n_rows: int = 2,\n",
    "    n_cols: int = 3,\n",
    "    individual_figsize: tuple[int, int] = (4, 4),\n",
    "    save_path: str | None = None,\n",
    "    ylim: tuple[float | None, float | None] = (None, None),\n",
    "    show_legend: bool = True,\n",
    "    legend_kwargs: dict = {},\n",
    "    colors: list[str] = default_colors,\n",
    "    use_inv_spearman: bool = False,\n",
    ") -> list[list[plt.Line2D]]:\n",
    "    num_metrics = len(metric_names)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_rows,\n",
    "        ncols=n_cols,\n",
    "        figsize=(individual_figsize[0] * n_cols, individual_figsize[1] * n_rows),\n",
    "    )\n",
    "    legend_handles = []\n",
    "    # Handle the case where axes might be a single element or already a list\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif hasattr(axes, \"flatten\"):  # Check if axes has flatten method\n",
    "        axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "    for i, (ax, metric_name) in enumerate(zip(axes, metric_names)):\n",
    "        metrics_dict = all_metrics_dict[metric_name]\n",
    "        for j, (model_name, metrics) in enumerate(metrics_dict.items()):\n",
    "            print(model_name)\n",
    "            print(metrics.keys())\n",
    "            mean_vals = np.array(metrics[\"means\"])\n",
    "            median_vals = np.array(metrics[\"medians\"])\n",
    "            if metric_name == \"spearman\" and use_inv_spearman:\n",
    "                mean_vals = 1 - mean_vals\n",
    "                median_vals = 1 - median_vals\n",
    "            ax.plot(\n",
    "                metrics[\"prediction_lengths\"],\n",
    "                median_vals,\n",
    "                marker=markers[j],\n",
    "                label=model_name,\n",
    "                markersize=6,\n",
    "                color=colors[j],\n",
    "                # alpha=0.8,\n",
    "            )\n",
    "            # std_envelope = np.array(metrics[\"stds\"])\n",
    "            se_envelope = np.array(metrics[\"stds\"]) / np.sqrt(len(metrics[\"stds\"]))\n",
    "            if metric_name in metrics_to_show_std_envelope:\n",
    "                ax.fill_between(\n",
    "                    metrics[\"prediction_lengths\"],\n",
    "                    mean_vals - se_envelope,\n",
    "                    mean_vals + se_envelope,\n",
    "                    alpha=0.1,\n",
    "                    color=colors[j],\n",
    "                )\n",
    "        if i == 0:\n",
    "            legend_handles = [\n",
    "                plt.Line2D(\n",
    "                    [0],\n",
    "                    [0],\n",
    "                    color=colors[j],\n",
    "                    marker=markers[j],\n",
    "                    markersize=6,\n",
    "                    label=model_name,\n",
    "                )\n",
    "                for j, model_name in enumerate(metrics_dict.keys())\n",
    "            ]\n",
    "            if show_legend:\n",
    "                legend_handles = ax.legend(handles=legend_handles, **legend_kwargs)\n",
    "        ax.set_xlabel(\"Prediction Length\", fontweight=\"bold\", fontsize=12)\n",
    "        ax.set_xticks(metrics[\"prediction_lengths\"])\n",
    "        name = metric_name.replace(\"_\", \" \")\n",
    "        if name in [\"mse\", \"mae\", \"rmse\", \"mape\"]:\n",
    "            name = name.upper()\n",
    "        elif name == \"smape\":\n",
    "            name = \"sMAPE\"\n",
    "        elif name == \"spearman\" and use_inv_spearman:\n",
    "            name = \"1 - Spearman\"\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        ax.set_title(name, fontweight=\"bold\", fontsize=16)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for ax in axes[num_metrics:]:\n",
    "        ax.set_visible(False)\n",
    "    if ylim is not None:\n",
    "        for ax in axes:\n",
    "            ax.set_ylim(ylim)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    return legend_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    metrics_to_show_std_envelope=[\"smape\", \"pearson\", \"spearman\"],\n",
    "    n_cols=4,\n",
    "    n_rows=1,\n",
    "    save_path=\"baselines_figs/zeroshot_metrics_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"loc\": \"upper left\", \"frameon\": True, \"fontsize\": 10},\n",
    "    colors=default_colors,\n",
    "    use_inv_spearman=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=5,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"baselines_figs/baselines_legend_horizontal.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"smape\"],\n",
    "    metrics_to_show_std_envelope=[\"smape\"],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    ylim=(10, None),\n",
    "    save_path=\"zeroshot_smape_autoregressive_rollout.pdf\",\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"spearman\"\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [selected_metric],\n",
    "    metrics_to_show_std_envelope=[\"smape\", \"spearman\"],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4),\n",
    "    save_path=f\"baselines_figs/baselines_{selected_metric}.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    use_inv_spearman=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
