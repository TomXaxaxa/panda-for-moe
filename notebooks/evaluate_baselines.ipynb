{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from dystformer.utils import (\n",
    "    apply_custom_style,\n",
    "    make_box_plot,\n",
    "    plot_all_metrics_by_prediction_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply matplotlib style from config\n",
    "apply_custom_style(\"../config/plotting.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../figures\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"test_zeroshot\"\n",
    "\n",
    "run_metrics_dir_dict = {\n",
    "    \"Our Model\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"patchtst\",\n",
    "        # \"pft_stand_rff_only_pretrained-0\",\n",
    "        # \"pft_chattn_noembed_pretrained_correct-0\",\n",
    "        \"pft_chattn_emb_w_poly-0\",\n",
    "        # \"pft_linattnpolyemb_from_scratch-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M SFT\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        \"chronos_bolt_mini-12\",\n",
    "        # \"chronos_mini_ft-0\",\n",
    "        # \"chronos_finetune_stand_updated-0\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Chronos 20M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"chronos\",\n",
    "        \"chronos_mini_zeroshot\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"Time MOE 50M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"timemoe\",\n",
    "        \"timemoe-50m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    \"TimesFM 200M\": os.path.join(\n",
    "        WORK_DIR,\n",
    "        \"eval_results\",\n",
    "        \"timesfm\",\n",
    "        \"timesfm-200m\",\n",
    "        data_split,\n",
    "    ),\n",
    "    # \"Mean\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"mean\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "    # \"Fourier\": os.path.join(\n",
    "    #     WORK_DIR,\n",
    "    #     \"eval_results\",\n",
    "    #     \"baselines\",\n",
    "    #     \"fourier\",\n",
    "    #     data_split,\n",
    "    # ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dir_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs = defaultdict(dict)\n",
    "for model_name, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "    print(model_name)\n",
    "    if not os.path.exists(run_metrics_dir):\n",
    "        print(f\"Run metrics dir does not exist: {run_metrics_dir}\")\n",
    "        continue\n",
    "    for file in sorted(\n",
    "        os.listdir(run_metrics_dir),\n",
    "        key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "    ):\n",
    "        if file.endswith(\".csv\"):\n",
    "            prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "            # print(prediction_length)\n",
    "            with open(os.path.join(run_metrics_dir, file), \"r\") as f:\n",
    "                metrics = pd.read_csv(f).to_dict()\n",
    "                metrics_all_runs[model_name][prediction_length] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all_runs[\"Our Model\"][64].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics = defaultdict(dict)\n",
    "for model_name, all_metrics_of_model in metrics_all_runs.items():\n",
    "    print(model_name)\n",
    "    for prediction_length, metrics in all_metrics_of_model.items():\n",
    "        systems = metrics[\"system\"]\n",
    "        metrics_unrolled = {\n",
    "            k: list(v.values()) for k, v in metrics.items() if k != \"system\"\n",
    "        }\n",
    "        # print(metrics_unrolled.keys())\n",
    "        unrolled_metrics[model_name][prediction_length] = metrics_unrolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Our Model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Our Model\"][64].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unrolled_metrics[\"Our Model\"][64][\"smape\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics[\"Our Model\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = len(unrolled_metrics.keys())\n",
    "print(n_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "default_colors = default_colors[: n_runs + 1]\n",
    "default_colors = default_colors[:3] + default_colors[4:]\n",
    "print(default_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metric = \"smape\"\n",
    "legend_handles = make_box_plot(\n",
    "    unrolled_metrics=unrolled_metrics,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=selected_metric,  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=default_colors,\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=f\"baselines_figs/{selected_metric}_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    "    box_percentile_range=(25, 75),\n",
    "    whisker_percentile_range=(5, 95),\n",
    "    alpha_val=0.8,\n",
    "    fig_kwargs={\"figsize\": (2, 4)},\n",
    "    box_width=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=5,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\n",
    "    \"baselines_figs/baselines_legend_horizontal_patches.pdf\", bbox_inches=\"tight\"\n",
    ")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_metrics_dict(\n",
    "    unrolled_metrics: dict, metric_name: str\n",
    ") -> dict[str, dict[str, list[float]]]:\n",
    "    summary_metrics_dict = defaultdict(dict)\n",
    "    for model_name, metrics_dict in unrolled_metrics.items():\n",
    "        print(model_name)\n",
    "        prediction_lengths = list(metrics_dict.keys())\n",
    "        summary_metrics_dict[model_name][\"prediction_lengths\"] = prediction_lengths\n",
    "        num_vals = len(metrics_dict[prediction_lengths[0]][metric_name])\n",
    "        summary_metrics_dict[model_name][\"num_vals\"] = num_vals\n",
    "        means = []\n",
    "        medians = []\n",
    "        stds = []\n",
    "        stes = []\n",
    "        all_vals = []\n",
    "        for prediction_length in prediction_lengths:\n",
    "            metric_vals = np.array(metrics_dict[prediction_length][metric_name])\n",
    "            # Filter out NaN values\n",
    "            filtered_vals = metric_vals[~np.isnan(metric_vals)]\n",
    "            assert len(metric_vals) == num_vals\n",
    "            means.append(np.mean(filtered_vals) if len(filtered_vals) > 0 else np.nan)\n",
    "            medians.append(\n",
    "                np.median(filtered_vals) if len(filtered_vals) > 0 else np.nan\n",
    "            )\n",
    "            curr_std = np.std(filtered_vals) if len(filtered_vals) > 0 else np.nan\n",
    "            stds.append(curr_std)\n",
    "            stes.append(\n",
    "                curr_std / np.sqrt(len(filtered_vals))\n",
    "                if len(filtered_vals) > 0\n",
    "                else np.nan\n",
    "            )\n",
    "            all_vals.append(filtered_vals)\n",
    "        summary_metrics_dict[model_name][\"means\"] = means\n",
    "        summary_metrics_dict[model_name][\"medians\"] = medians\n",
    "        summary_metrics_dict[model_name][\"stds\"] = stds\n",
    "        summary_metrics_dict[model_name][\"stes\"] = stes\n",
    "        summary_metrics_dict[model_name][\"all_vals\"] = all_vals  # list of numpy arrays\n",
    "    return summary_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_metrics_dict = get_summary_metrics_dict(unrolled_metrics, \"smape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict = {\n",
    "    metrics_name: get_summary_metrics_dict(unrolled_metrics, metrics_name)\n",
    "    for metrics_name in [\n",
    "        \"mse\",\n",
    "        \"mae\",\n",
    "        \"smape\",\n",
    "        \"spearman\",\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order model names by sMAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names_ordering = []  # sorted by median smape at 128\n",
    "for model_name, data in all_metrics_dict[\"smape\"].items():\n",
    "    print(model_name)\n",
    "    median_metrics_128 = data[\"medians\"][1]\n",
    "    model_names_ordering.append((model_name, median_metrics_128))\n",
    "model_names_ordering = sorted(model_names_ordering, key=lambda x: x[1])\n",
    "model_names_ordering = [x[0] for x in model_names_ordering]\n",
    "print(model_names_ordering)\n",
    "\n",
    "# Reorder all_metrics_dict according to model_names_ordering for each metric\n",
    "reordered_metrics_dict = {}\n",
    "for metric_name, metric_data in all_metrics_dict.items():\n",
    "    reordered_metric_data = {}\n",
    "\n",
    "    # Add models in the order specified by model_names_ordering\n",
    "    for model_name in model_names_ordering:\n",
    "        if model_name in metric_data:\n",
    "            reordered_metric_data[model_name] = metric_data[model_name]\n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} not found in {metric_name}\")\n",
    "\n",
    "    reordered_metrics_dict[metric_name] = reordered_metric_data\n",
    "all_metrics_dict = reordered_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict[\"mse\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict[\"smape\"][\"Our Model\"][\"num_vals\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_metrics_dict[\"smape\"][\"Our Model\"][\"all_vals\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    metrics_to_show_envelope=[\"mae\", \"smape\", \"spearman\"],\n",
    "    n_cols=4,\n",
    "    n_rows=1,\n",
    "    save_path=\"baselines_figs/zeroshot_metrics_autoregressive_rollout_metrics.pdf\",\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"loc\": \"upper left\", \"frameon\": True, \"fontsize\": 10},\n",
    "    colors=default_colors,\n",
    "    use_inv_spearman=True,\n",
    "    percentile_range=(40, 60),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 1))\n",
    "\n",
    "# Add the legend with the combined handles\n",
    "legend = plt.legend(\n",
    "    handles=legend_handles,\n",
    "    loc=\"upper center\",\n",
    "    frameon=True,\n",
    "    ncol=5,\n",
    "    framealpha=1.0,\n",
    "    fontsize=16,\n",
    ")\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"baselines_figs/baselines_legend_horizontal.pdf\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"smape\"],\n",
    "    metrics_to_show_envelope=[\"smape\"],\n",
    "    n_cols=1,\n",
    "    n_rows=1,\n",
    "    individual_figsize=(4, 4.5),\n",
    "    ylim=(10, None),\n",
    "    save_path=\"zeroshot_smape_autoregressive_rollout_metrics.pdf\",\n",
    "    legend_kwargs={\"frameon\": True, \"fontsize\": 10, \"loc\": \"lower right\"},\n",
    "    colors=default_colors,\n",
    "    percentile_range=(30, 70),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict[\"smape\"].items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra: Plot the Mean and STE Envelope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_handles = plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    metrics_to_show_envelope=[\"mae\", \"smape\", \"spearman\"],\n",
    "    stat_to_plot=\"mean\",\n",
    "    n_cols=4,\n",
    "    n_rows=1,\n",
    "    show_legend=False,\n",
    "    legend_kwargs={\"loc\": \"upper left\", \"frameon\": True, \"fontsize\": 10},\n",
    "    colors=default_colors,\n",
    "    use_inv_spearman=True,\n",
    "    percentile_range=(40, 60),\n",
    "    # model_names_to_exclude=[\"TimesFM 200M\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
