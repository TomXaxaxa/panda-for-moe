{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"../custom_style.mplstyle\"):\n",
    "    plt.style.use([\"ggplot\", \"../custom_style.mplstyle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_save_dir = os.path.join(\"../figs\", \"eval_metrics\")\n",
    "os.makedirs(figs_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = os.getenv(\"WORK\", \"\")\n",
    "DATA_DIR = os.path.join(WORK_DIR, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_split = \"final_skew40/test_zeroshot\"\n",
    "\n",
    "run_names_chattn = {\n",
    "    \"chattn + mlm + embedding\": \"pft_stand_rff_only_pretrained\",\n",
    "    \"chattn + mlm\": \"pft_chattn_noembed_pretrained_correct\",\n",
    "    # \"mlm + chattn\": \"pft_chattn_noemb_pretrained_chrope\",\n",
    "    \"chattn + embedding\": \"pft_fullyfeat_from_scratch\",\n",
    "    # \"chattn + embedding\": \"pft_fullyfeat_from_scratch_longer\",\n",
    "    \"chattn\": \"pft_stand_chattn_noemb\",\n",
    "}\n",
    "\n",
    "run_names_no_chattn = {\n",
    "    \"vanilla (wider) + embedding\": \"pft_emb_equal_param_univariate_from_scratch\",\n",
    "    \"vanilla (wider)\": \"pft_noemb_equal_param_univariate_from_scratch\",\n",
    "    \"vanilla (deeper)\": \"pft_equal_param_deeper_univariate_from_scratch_noemb\",\n",
    "    \"vanilla + mlm + embedding\": \"pft_rff_univariate_pretrained\",\n",
    "    \"vanilla + mlm\": \"pft_vanilla_pretrained_correct\",\n",
    "}\n",
    "\n",
    "run_names = {\n",
    "    **run_names_chattn,\n",
    "    **run_names_no_chattn,\n",
    "}\n",
    "\n",
    "run_metrics_dirs_all_groups = {\n",
    "    \"chattn\": {\n",
    "        run_abbrv: os.path.join(\n",
    "            \"../eval_results\",\n",
    "            \"patchtst\",\n",
    "            f\"{run_name}-0\",\n",
    "            data_split,\n",
    "        )\n",
    "        for run_abbrv, run_name in run_names_chattn.items()\n",
    "    },\n",
    "    \"no_chattn\": {\n",
    "        run_abbrv: os.path.join(\n",
    "            \"../eval_results\",\n",
    "            \"patchtst\",\n",
    "            f\"{run_name}-0\",\n",
    "            data_split,\n",
    "        )\n",
    "        for run_abbrv, run_name in run_names_no_chattn.items()\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dirs_all_groups[\"no_chattn\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all = defaultdict(lambda: defaultdict(dict))\n",
    "for run_group, run_metrics_dir_dict in run_metrics_dirs_all_groups.items():\n",
    "    print(f\"Run group: {run_group}\")\n",
    "    for run_abbrv, run_metrics_dir in run_metrics_dir_dict.items():\n",
    "        run_abbrv = str(run_abbrv)\n",
    "        print(f\"{run_abbrv}: {run_metrics_dir}\")\n",
    "        for file in sorted(\n",
    "            os.listdir(run_metrics_dir),\n",
    "            key=lambda x: int(x.split(\"_pred\")[1].split(\".csv\")[0]),\n",
    "        ):\n",
    "            if file.endswith(\".csv\"):\n",
    "                prediction_length = int(file.split(\"_pred\")[1].split(\".csv\")[0])\n",
    "                # print(f\"Prediction length: {prediction_length} for {run_abbrv}\")\n",
    "                with open(os.path.join(run_metrics_dir, file), \"r\") as f:\n",
    "                    metrics = pd.read_csv(f).to_dict()\n",
    "                    metrics_all[run_group][run_abbrv][prediction_length] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all[\"no_chattn\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_groups = defaultdict(lambda: defaultdict(dict))\n",
    "for run_group, all_metrics_of_run_group in metrics_all.items():\n",
    "    print(run_group)\n",
    "    for run_abbrv, all_metrics_of_run_abbrv in all_metrics_of_run_group.items():\n",
    "        print(run_abbrv)\n",
    "        for run_name, metrics in all_metrics_of_run_abbrv.items():\n",
    "            print(run_name)\n",
    "            systems = metrics.pop(\"system\")\n",
    "            metrics_unrolled = {k: list(v.values()) for k, v in metrics.items()}\n",
    "            print(metrics_unrolled.keys())\n",
    "            unrolled_metrics_all_groups[run_group][run_abbrv][run_name] = (\n",
    "                metrics_unrolled\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_combined = {\n",
    "    **unrolled_metrics_all_groups[\"chattn\"],\n",
    "    **unrolled_metrics_all_groups[\"no_chattn\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_groups[\"no_chattn\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_metrics_dict(unrolled_metrics, metric_name):\n",
    "    summary_metrics_dict = defaultdict(dict)\n",
    "    for model_name, metrics_dict in unrolled_metrics.items():\n",
    "        print(model_name)\n",
    "        prediction_lengths = list(metrics_dict.keys())\n",
    "        summary_metrics_dict[model_name][\"prediction_lengths\"] = prediction_lengths\n",
    "        means = []\n",
    "        medians = []\n",
    "        stds = []\n",
    "        for prediction_length in prediction_lengths:\n",
    "            metric_val = metrics_dict[prediction_length][metric_name]\n",
    "            means.append(np.nanmean(metric_val))\n",
    "            medians.append(np.nanmedian(metric_val))\n",
    "            stds.append(np.nanstd(metric_val))\n",
    "        summary_metrics_dict[model_name][\"means\"] = means\n",
    "        summary_metrics_dict[model_name][\"medians\"] = medians\n",
    "        summary_metrics_dict[model_name][\"stds\"] = stds\n",
    "    return summary_metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_by_prediction_length(\n",
    "    metrics_dict, metric_name, show_std_envelope=False\n",
    "):\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    for model_name, metrics in metrics_dict.items():\n",
    "        plt.plot(\n",
    "            metrics[\"prediction_lengths\"],\n",
    "            metrics[\"medians\"],\n",
    "            marker=\"o\",\n",
    "            label=model_name,\n",
    "        )\n",
    "        std_envelope = np.array(metrics[\"stds\"])\n",
    "        if show_std_envelope:\n",
    "            plt.fill_between(\n",
    "                metrics[\"prediction_lengths\"],\n",
    "                metrics[\"means\"] - std_envelope,\n",
    "                metrics[\"means\"] + std_envelope,\n",
    "                alpha=0.2,\n",
    "            )\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"Prediction Length\")\n",
    "    plt.title(metric_name, fontweight=\"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_metrics_dirs_all_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names_chosen = [\n",
    "    \"mse\",\n",
    "    \"mae\",\n",
    "    \"smape\",\n",
    "    \"r2_score\",\n",
    "    \"spearman\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict = defaultdict(dict)\n",
    "\n",
    "for run_group in run_metrics_dirs_all_groups.keys():\n",
    "    all_metrics_dict[run_group] = {\n",
    "        metrics_name: get_summary_metrics_dict(\n",
    "            unrolled_metrics_all_groups[run_group], metrics_name\n",
    "        )\n",
    "        for metrics_name in metric_names_chosen\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "\n",
    "def plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict: dict[str, dict[str, dict[str, list[float]]]],\n",
    "    metric_names: list[str],\n",
    "    runs_to_exclude: list[str],\n",
    "    metrics_to_show_std_envelope: list[str],\n",
    "    n_rows: int = 2,\n",
    "    n_cols: int = 3,\n",
    "    limit_num_prediction_lengths: int | None = None,\n",
    "    title: str = \"\",\n",
    "):\n",
    "    num_metrics = len(metric_names)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=n_rows, ncols=n_cols, figsize=(4 * n_cols, 4 * n_rows)\n",
    "    )\n",
    "    axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
    "\n",
    "    for i, (ax, metric_name) in enumerate(zip(axes, metric_names)):\n",
    "        metrics_dict = all_metrics_dict[metric_name]\n",
    "        for j, (model_name, metrics) in enumerate(metrics_dict.items()):\n",
    "            if model_name in runs_to_exclude:\n",
    "                continue\n",
    "            # print(model_name)\n",
    "            # print(metrics.keys())\n",
    "            prediction_lengths = metrics[\"prediction_lengths\"][\n",
    "                :limit_num_prediction_lengths\n",
    "            ]\n",
    "            medians = metrics[\"medians\"][:limit_num_prediction_lengths]\n",
    "            means = metrics[\"means\"][:limit_num_prediction_lengths]\n",
    "            stds = metrics[\"stds\"][:limit_num_prediction_lengths]\n",
    "            ax.plot(\n",
    "                prediction_lengths,\n",
    "                medians,\n",
    "                marker=\"o\",\n",
    "                label=model_name,\n",
    "                color=colors[j],\n",
    "                alpha=0.7,\n",
    "            )\n",
    "            std_envelope = np.array(stds)\n",
    "            if metric_name in metrics_to_show_std_envelope:\n",
    "                ax.fill_between(\n",
    "                    prediction_lengths,\n",
    "                    means - std_envelope,\n",
    "                    means + std_envelope,\n",
    "                    alpha=0.2,\n",
    "                    color=colors[j],\n",
    "                )\n",
    "        if i == 0:\n",
    "            ax.legend(loc=\"lower right\", fontsize=10, frameon=True)\n",
    "        ax.set_xlabel(\"Prediction Length\")\n",
    "        ax.set_xticks(prediction_lengths)\n",
    "        name = metric_name.replace(\"_\", \" \")\n",
    "        if name in [\"mse\", \"mae\", \"rmse\", \"mape\", \"smape\"]:\n",
    "            name = name.upper()\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        ax.set_title(name, fontweight=\"bold\")\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for ax in axes[num_metrics:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title, fontweight=\"bold\", fontsize=16, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict[\"no_chattn\"],\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    # metrics_to_show_std_envelope=[\"smape\", \"pearson\", \"spearman\"],\n",
    "    runs_to_exclude=[],\n",
    "    metrics_to_show_std_envelope=[],\n",
    "    limit_num_prediction_lengths=None,\n",
    "    title=\"No Channel Attention\",\n",
    "    n_cols=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict[\"chattn\"],\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    # metrics_to_show_std_envelope=[\"smape\", \"pearson\", \"spearman\"],\n",
    "    runs_to_exclude=[],\n",
    "    metrics_to_show_std_envelope=[],\n",
    "    limit_num_prediction_lengths=None,\n",
    "    title=\"Channel Attention\",\n",
    "    n_cols=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics_dict_all = {\n",
    "    metrics_name: {\n",
    "        **all_metrics_dict[\"chattn\"][metrics_name],\n",
    "        **all_metrics_dict[\"no_chattn\"][metrics_name],\n",
    "    }\n",
    "    for metrics_name in metric_names_chosen\n",
    "}\n",
    "plot_all_metrics_by_prediction_length(\n",
    "    all_metrics_dict_all,\n",
    "    [\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    # metrics_to_show_std_envelope=[\"smape\", \"pearson\", \"spearman\"],\n",
    "    runs_to_exclude=[],\n",
    "    metrics_to_show_std_envelope=[],\n",
    "    limit_num_prediction_lengths=None,\n",
    "    title=\"All Models\",\n",
    "    n_cols=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def make_aggregate_plot(\n",
    "    unrolled_metrics: dict[str, dict[int, dict[str, list[float]]]],\n",
    "    prediction_length: int,\n",
    "    selected_run_names: list[str] | None = None,\n",
    "    ylim: tuple[float, float] = (1e-5, 1e5),\n",
    "    verbose: bool = False,\n",
    "    metrics_to_include: list[str] = [],\n",
    "    run_names_to_exclude: list[str] = [],\n",
    "    use_rescaled_smape: bool = False,\n",
    "    use_inv_spearman: bool = False,\n",
    "    title: str = \"Metrics\",\n",
    "    fig_kwargs: dict[str, Any] = {},\n",
    "    legend_kwargs: dict[str, Any] = {},\n",
    "    title_kwargs: dict[str, Any] = {},\n",
    "    bar_kwargs: dict[str, Any] = {},\n",
    "    plot_type: str = \"box\",  # New parameter: 'box' or 'bar'\n",
    "    colors: list[str] | None = None,\n",
    "    sort_by_metric: str | None = None,\n",
    "    save_path: str | None = None,\n",
    "):\n",
    "    if colors is None:\n",
    "        colors = plt.cm.tab10.colors\n",
    "    # Extract metrics data for the given prediction_length and run_names\n",
    "    if selected_run_names is None:\n",
    "        selected_run_names = list(unrolled_metrics.keys())\n",
    "\n",
    "    metrics_by_run_name = {\n",
    "        run_name: unrolled_metrics[run_name][prediction_length]\n",
    "        for run_name in selected_run_names\n",
    "    }\n",
    "\n",
    "    run_names = list(metrics_by_run_name.keys())\n",
    "    metric_names = list(metrics_by_run_name[run_names[0]].keys())\n",
    "    metric_names = [name for name in metric_names if name in metrics_to_include]\n",
    "    run_names = [name for name in run_names if name not in run_names_to_exclude]\n",
    "\n",
    "    print(f\"run_names: {run_names}\")\n",
    "    print(f\"metric_names: {metric_names}\")\n",
    "\n",
    "    # Create pretty titles for x-axis tick labels\n",
    "    metric_names_title = []\n",
    "    for name in metric_names:\n",
    "        # Create pretty titles for x-axis tick labels\n",
    "        if name in [\"mse\", \"mae\", \"rmse\", \"mape\"]:\n",
    "            name = name.upper()\n",
    "        elif name == \"smape\":\n",
    "            name = \"sMAPE\"\n",
    "        elif name == \"spearman\":\n",
    "            name = \"1 - Spearman\"\n",
    "        else:\n",
    "            name = name.capitalize()\n",
    "        metric_names_title.append(name)\n",
    "\n",
    "    if fig_kwargs == {}:\n",
    "        fig_kwargs = {\"figsize\": (6, 4)}\n",
    "    plt.figure(**fig_kwargs)\n",
    "\n",
    "    plot_data = []\n",
    "    median_data = []\n",
    "    for i, metric_name in enumerate(metric_names):\n",
    "        metric_title = metric_names_title[i]\n",
    "        for run_name in run_names:\n",
    "            values = metrics_by_run_name[run_name][metric_name]\n",
    "            if metric_name == \"smape\" and use_rescaled_smape:\n",
    "                values = [x / 200 for x in values]\n",
    "            if metric_name == \"spearman\" and use_inv_spearman:\n",
    "                values = [1 - x for x in values]\n",
    "            median_value = np.nanmedian(values)\n",
    "            plot_data.extend([(metric_title, v, run_name) for v in values])\n",
    "            median_data.append((metric_title, median_value, run_name))\n",
    "            if verbose:\n",
    "                print(f\"{metric_title} - {run_name} median: {median_value}\")\n",
    "\n",
    "    # Create DataFrame for use with seaborn\n",
    "    df = pd.DataFrame(plot_data, columns=[\"Metric\", \"Value\", \"Run\"])\n",
    "\n",
    "    # Set the order of metrics to match the original order in metric_names_title\n",
    "    df[\"Metric\"] = pd.Categorical(\n",
    "        df[\"Metric\"], categories=metric_names_title, ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort runs based on the specified metric or overall median\n",
    "    if sort_by_metric is not None:\n",
    "        # Find the pretty title for the specified metric\n",
    "        sort_metric_title = None\n",
    "        for i, metric_name in enumerate(metric_names):\n",
    "            if metric_name.lower() == sort_by_metric.lower():\n",
    "                sort_metric_title = metric_names_title[i]\n",
    "                break\n",
    "\n",
    "        if sort_metric_title is None:\n",
    "            print(\n",
    "                f\"Warning: Metric '{sort_by_metric}' not found. Using overall median for sorting.\"\n",
    "            )\n",
    "            # Calculate the median for each run (across all metrics)\n",
    "            median_values = df.groupby(\"Run\")[\"Value\"].median().reset_index()\n",
    "        else:\n",
    "            # Filter data for the specific metric and calculate median\n",
    "            metric_df = df[df[\"Metric\"] == sort_metric_title]\n",
    "            median_values = metric_df.groupby(\"Run\")[\"Value\"].median().reset_index()\n",
    "    else:\n",
    "        # Calculate the median for each run (across all metrics)\n",
    "        median_values = df.groupby(\"Run\")[\"Value\"].median().reset_index()\n",
    "\n",
    "    # Sort the medians\n",
    "    sorted_medians = median_values.sort_values(\"Value\")\n",
    "\n",
    "    # Create a categorical type with the sorted run names to preserve the order\n",
    "    df[\"Run\"] = pd.Categorical(\n",
    "        df[\"Run\"], categories=sorted_medians[\"Run\"].tolist(), ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort the original dataframe\n",
    "    df = df.sort_values(\"Run\")\n",
    "\n",
    "    # Choose plot type based on parameter\n",
    "    if plot_type == \"box\":\n",
    "        # Plot box plot\n",
    "        sns.boxplot(\n",
    "            data=df,\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Run\",\n",
    "            dodge=True,\n",
    "            width=0.8,\n",
    "            fliersize=0,  # Don't show outlier points\n",
    "            palette=colors[: len(run_names)],\n",
    "            medianprops={\"linewidth\": 2.5, \"solid_capstyle\": \"butt\"},\n",
    "            saturation=0.6,\n",
    "        )\n",
    "    elif plot_type == \"bar\":\n",
    "        if bar_kwargs == {}:\n",
    "            bar_kwargs = {\"estimator\": \"median\"}\n",
    "        # Plot bar plot\n",
    "        sns.barplot(\n",
    "            data=df,\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Run\",\n",
    "            palette=colors[: len(run_names)],\n",
    "            saturation=0.6,\n",
    "            **bar_kwargs,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported plot_type: {plot_type}\")\n",
    "\n",
    "    plt.ylim(ylim)\n",
    "    plt.ylabel(\"\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.xticks(fontweight=\"bold\")\n",
    "    plt.legend(**legend_kwargs)\n",
    "    plt.title(title, fontweight=\"bold\", **title_kwargs)\n",
    "    plt.xticks(rotation=15)  # Optional: rotates x-tick labels for readability\n",
    "    plt.tight_layout()\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_aggregate_plot(\n",
    "#     unrolled_metrics_all_groups[\"no_chattn\"],\n",
    "#     128,\n",
    "#     selected_run_names=None,\n",
    "#     ylim=(0, 2),\n",
    "#     metrics_to_include=[\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "#     use_rescaled_smape=True,\n",
    "#     title=\"No Channel Attention\",\n",
    "#     legend_kwargs={\"loc\": \"upper right\", \"fontsize\": 5, \"frameon\": True},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_groups[\"chattn\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_aggregate_plot(\n",
    "#     unrolled_metrics_all_groups[\"chattn\"],\n",
    "#     128,\n",
    "#     selected_run_names=None,\n",
    "#     ylim=(0, 2),\n",
    "#     metrics_to_include=[\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "#     use_rescaled_smape=True,\n",
    "#     title=\"Channel Attention\",\n",
    "#     legend_kwargs={\"loc\": \"upper right\", \"fontsize\": 5, \"frameon\": True},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unrolled_metrics_all_combined.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_gray = (0.7, 0.7, 0.7)\n",
    "bar_colors = list(plt.cm.tab20c.colors[:8]) + [light_gray]\n",
    "print(len(bar_colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot(\n",
    "    unrolled_metrics_all_combined,\n",
    "    128,\n",
    "    selected_run_names=None,\n",
    "    ylim=(0, 0.45),\n",
    "    metrics_to_include=[\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    use_rescaled_smape=True,\n",
    "    use_inv_spearman=True,\n",
    "    title=\"Zeroshot Metrics for Ablations\",\n",
    "    legend_kwargs={\"loc\": \"upper right\", \"fontsize\": 5.5, \"frameon\": True},\n",
    "    plot_type=\"bar\",\n",
    "    bar_kwargs={\n",
    "        \"estimator\": \"median\",\n",
    "        \"errorbar\": (\"ci\", 95),\n",
    "        # \"errorbar\": (\"se\", 2),\n",
    "        \"err_kws\": {\"linewidth\": 0.8, \"alpha\": 0.8},\n",
    "    },\n",
    "    colors=bar_colors,\n",
    "    sort_by_metric=\"smape\",\n",
    "    save_path=\"ablations_figs/ablations_pred128.pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot(\n",
    "    unrolled_metrics_all_combined,\n",
    "    128,\n",
    "    selected_run_names=None,\n",
    "    ylim=(0, 1),\n",
    "    metrics_to_include=[\"mse\", \"mae\", \"smape\", \"spearman\"],\n",
    "    use_rescaled_smape=True,\n",
    "    use_inv_spearman=True,\n",
    "    title=\"Zeroshot Metrics for Ablations\",\n",
    "    legend_kwargs={\n",
    "        \"loc\": \"upper right\",\n",
    "        \"fontsize\": 6,\n",
    "        \"frameon\": True,\n",
    "        \"framealpha\": 1.0,\n",
    "    },\n",
    "    plot_type=\"box\",\n",
    "    colors=bar_colors,\n",
    "    sort_by_metric=\"smape\",\n",
    "    save_path=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... existing code ...\n",
    "def make_aggregate_plot_v2(\n",
    "    unrolled_metrics: dict[str, dict[int, dict[str, list[float]]]],\n",
    "    prediction_length: int,\n",
    "    metric_to_plot: str = \"smape\",  # Default to smape\n",
    "    selected_run_names: list[str] | None = None,\n",
    "    ylim: tuple[float, float] | None = None,  # Changed to None as default\n",
    "    verbose: bool = False,\n",
    "    run_names_to_exclude: list[str] = [],\n",
    "    use_rescaled_smape: bool = False,\n",
    "    use_inv_spearman: bool = False,\n",
    "    title: str | None = None,\n",
    "    fig_kwargs: dict[str, Any] = {},\n",
    "    title_kwargs: dict[str, Any] = {},\n",
    "    colors: list[str] | None = None,\n",
    "    sort_runs: bool = False,\n",
    "    save_path: str | None = None,\n",
    "    order_by_metric: str | None = None,\n",
    "    ylabel_fontsize: int = 8,\n",
    "    show_xlabel: bool = True,\n",
    "):\n",
    "    # Set default figure size if not provided\n",
    "    if fig_kwargs == {}:\n",
    "        fig_kwargs = {\"figsize\": (3, 5)}  # Wider figure to accommodate run names\n",
    "\n",
    "    if colors is None:\n",
    "        colors = plt.cm.tab10.colors\n",
    "\n",
    "    # Extract metrics data for the given prediction_length and run_names\n",
    "    if selected_run_names is None:\n",
    "        selected_run_names = list(unrolled_metrics.keys())\n",
    "\n",
    "    # Filter out excluded run names\n",
    "    run_names = [\n",
    "        name for name in selected_run_names if name not in run_names_to_exclude\n",
    "    ]\n",
    "\n",
    "    if len(run_names) == 0:\n",
    "        print(\"No run names to plot after exclusions!\")\n",
    "        return\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure(**fig_kwargs)\n",
    "    # Collect data for plotting\n",
    "    plot_data = []\n",
    "\n",
    "    # Add a dictionary to store ordering metric values if needed\n",
    "    ordering_metric_data = {}\n",
    "\n",
    "    for run_name in run_names:\n",
    "        try:\n",
    "            # Check if this run has data for the specified prediction length\n",
    "            if prediction_length not in unrolled_metrics[run_name]:\n",
    "                print(\n",
    "                    f\"Warning: prediction_length {prediction_length} not found for {run_name}\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Check if this run has the specified metric\n",
    "            if metric_to_plot not in unrolled_metrics[run_name][prediction_length]:\n",
    "                print(f\"Warning: metric '{metric_to_plot}' not found for {run_name}\")\n",
    "                continue\n",
    "\n",
    "            values = unrolled_metrics[run_name][prediction_length][metric_to_plot]\n",
    "\n",
    "            # Process values based on metric type\n",
    "            if metric_to_plot == \"smape\" and use_rescaled_smape:\n",
    "                values = [x / 200 for x in values]\n",
    "            if metric_to_plot == \"spearman\" and use_inv_spearman:\n",
    "                values = [1 - x for x in values]\n",
    "\n",
    "            # Filter out NaN values\n",
    "            values = [v for v in values if not np.isnan(v)]\n",
    "\n",
    "            if len(values) == 0:\n",
    "                print(f\"Warning: All values for {run_name} are NaN\")\n",
    "                continue\n",
    "\n",
    "            median_value = np.median(values)\n",
    "            plot_data.extend([(run_name, v) for v in values])\n",
    "\n",
    "            # If we need to order by a different metric, collect that data too\n",
    "            if order_by_metric is not None and order_by_metric != metric_to_plot:\n",
    "                if order_by_metric in unrolled_metrics[run_name][prediction_length]:\n",
    "                    order_values = unrolled_metrics[run_name][prediction_length][\n",
    "                        order_by_metric\n",
    "                    ]\n",
    "\n",
    "                    # Apply same processing as we would for the plotting metric\n",
    "                    if order_by_metric == \"smape\" and use_rescaled_smape:\n",
    "                        order_values = [x / 200 for x in order_values]\n",
    "                    if order_by_metric == \"spearman\" and use_inv_spearman:\n",
    "                        order_values = [1 - x for x in order_values]\n",
    "\n",
    "                    # Filter out NaN values\n",
    "                    order_values = [v for v in order_values if not np.isnan(v)]\n",
    "\n",
    "                    if order_values:\n",
    "                        ordering_metric_data[run_name] = np.median(order_values)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"{run_name} median {metric_to_plot}: {median_value}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {run_name}: {e}\")\n",
    "\n",
    "    # ... existing code ...\n",
    "\n",
    "    # Create DataFrame for seaborn\n",
    "    df = pd.DataFrame(plot_data, columns=[\"Run\", \"Value\"])\n",
    "\n",
    "    # Determine run order based on specified criteria\n",
    "    if order_by_metric is not None and ordering_metric_data:\n",
    "        # Sort runs based on the ordering metric's median values\n",
    "        run_order = [\n",
    "            run for run, _ in sorted(ordering_metric_data.items(), key=lambda x: x[1])\n",
    "        ]\n",
    "        # Only include runs that are in our dataframe\n",
    "        run_order = [run for run in run_order if run in df[\"Run\"].unique()]\n",
    "        # Set categorical order for Run column\n",
    "        df[\"Run\"] = pd.Categorical(df[\"Run\"], categories=run_order, ordered=True)\n",
    "    elif sort_runs:\n",
    "        # Use the existing sort_runs logic if order_by_metric isn't specified\n",
    "        median_by_run = df.groupby(\"Run\")[\"Value\"].median().sort_values()\n",
    "        run_order = median_by_run.index.tolist()\n",
    "        df[\"Run\"] = pd.Categorical(df[\"Run\"], categories=run_order, ordered=True)\n",
    "\n",
    "    # ... rest of existing code ...\n",
    "    # Create title for the metric\n",
    "    metric_title = metric_to_plot\n",
    "    if metric_to_plot in [\"mse\", \"mae\", \"rmse\", \"mape\"]:\n",
    "        metric_title = metric_to_plot.upper()\n",
    "    elif metric_to_plot == \"smape\":\n",
    "        metric_title = \"sMAPE\"\n",
    "    elif metric_to_plot == \"spearman\":\n",
    "        metric_title = \"1 - Spearman\" if use_inv_spearman else \"Spearman\"\n",
    "    else:\n",
    "        metric_title = metric_to_plot.capitalize()\n",
    "\n",
    "    # Plot the data\n",
    "    ax = sns.boxplot(\n",
    "        data=df,\n",
    "        x=\"Run\",\n",
    "        y=\"Value\",\n",
    "        width=0.6,\n",
    "        fliersize=0,\n",
    "        medianprops={\"linewidth\": 2.5, \"solid_capstyle\": \"butt\"},\n",
    "        boxprops={\"alpha\": 0.7},\n",
    "        palette=colors[: len(df[\"Run\"].unique())],\n",
    "        whis=(0, 90),\n",
    "        # log_scale=True,\n",
    "    )\n",
    "\n",
    "    # Set y-limits if provided\n",
    "    if ylim:\n",
    "        plt.ylim(ylim)\n",
    "\n",
    "    # Format the plot\n",
    "    plt.ylabel(metric_title, fontweight=\"bold\", fontsize=ylabel_fontsize)\n",
    "    plt.xlabel(\"\")\n",
    "    if show_xlabel:\n",
    "        # Format x-axis labels\n",
    "        plt.xticks(rotation=45, ha=\"right\", fontsize=5, fontweight=\"bold\")\n",
    "    else:\n",
    "        plt.xticks([])\n",
    "\n",
    "    # Set the title\n",
    "    if title is not None:\n",
    "        title_with_metric = f\"{title}: {metric_title}\" if title == \"Metrics\" else title\n",
    "        plt.title(title_with_metric, fontweight=\"bold\", **title_kwargs)\n",
    "\n",
    "    # Ensure plot is properly displayed\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save if path provided\n",
    "    if save_path is not None:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=300)\n",
    "\n",
    "    # Add a legend to the right side of the plot\n",
    "    # Place the legend outside the plot to the right\n",
    "    # plt.legend(\n",
    "    #     bbox_to_anchor=(1.05, 1),\n",
    "    #     loc=\"upper right\",\n",
    "    #     borderaxespad=0.0,\n",
    "    #     frameon=True,\n",
    "    # )\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "    return df  # Return the DataFrame for further analysis if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot_v2(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=\"smape\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    ylim=(25, 110),\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    save_path=\"ablations_figs/smape_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot_v2(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=\"spearman\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    ylim=(0.06, 0.6),\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=\"ablations_figs/spearman_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot_v2(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=\"mae\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    ylim=(0.14, 0.78),\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=\"ablations_figs/mae_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_aggregate_plot_v2(\n",
    "    unrolled_metrics=unrolled_metrics_all_combined,\n",
    "    prediction_length=128,\n",
    "    metric_to_plot=\"mse\",  # Specify which metric to plot\n",
    "    sort_runs=True,  # Optionally sort runs by their metric values\n",
    "    colors=bar_colors,\n",
    "    ylim=(0.06, 1.29),\n",
    "    title=None,\n",
    "    title_kwargs={\"fontsize\": 10},\n",
    "    use_inv_spearman=True,\n",
    "    order_by_metric=\"smape\",\n",
    "    save_path=\"ablations_figs/mse_128.pdf\",\n",
    "    ylabel_fontsize=12,\n",
    "    show_xlabel=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dystformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
