train:
  seed: 99

  # train, checkpoint, log steps
  max_steps: 100_000
  save_steps: 10_000
  log_steps: 100

  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  dataloader_num_workers: 16
  dataloader_prefetch_factor: 4

  tf32: false # NOTE: tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7
  torch_compile: true

  # optimizer
  optim: adamw_torch_fused
  learning_rate: 1e-3
  lr_scheduler_type: linear
  warmup_ratio: 0.0

  output_dir: /stor/work/AMDG_Gilpin_Summer2024/checkpoints/

  # not currently used, but part of TrainingArguments
  ddp_find_unused_parameters: false
  remove_unused_columns: false