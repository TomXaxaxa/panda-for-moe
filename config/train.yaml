train:
  seed: 99
  ddp_backend: nccl
  # train, checkpoint, log steps
  max_steps: 100_000
  save_steps: 10_000
  log_steps: 100

  per_device_train_batch_size: 8
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  dataloader_num_workers: 16
  dataloader_prefetch_factor: 2

  tf32: false # NOTE: tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7
  torch_compile: true

  # optimizer
  optim: adamw_torch_fused
  learning_rate: 1e-3
  lr_scheduler_type: linear
  warmup_ratio: 0.0

  output_dir: /stor/work/AMDG_Gilpin_Summer2024/checkpoints/

  # not currently used, but part of TrainingArguments
  ddp_find_unused_parameters: false
  remove_unused_columns: false


quantizer:
  enabled: false
  step_interval: 10_000
  num_bins_growth_factor: 2
  initial_bins: 100
  max_bins: 10_000

noiser:
  enabled: false
  schedule_name: cosine
  start: 1.0
  end: 1e-3
  decay_rate: 8.0
  eps: 0.008
  log_steps: 100
  epoch_stop: 0.5
