# PatchTST parameters
patchtst:
  mode: "predict"

  context_length: 512
  prediction_length: 64
  distribution_output: "student_t"
  loss: "mse"
  patch_length: 16
  patch_stride: 16
  num_hidden_layers: 8
  d_model: 512
  num_attention_heads: 8
  channel_attention: true # This mixes across channels in the encoder 
  ffn_dim: 512
  norm_type: "layernorm"
  norm_eps: 1e-05
  attention_dropout: 0.0
  positional_dropout: 0.0
  path_dropout: 0.0
  ff_dropout: 0.0
  bias: true
  activation_function: "gelu"
  pre_norm: true
  use_cls_token: false
  init_std: 0.02
  scaling: "std"
  do_mask_input: null
  mask_type: "random"
  random_mask_ratio: 0.5
  num_forecast_mask_patches: 3
  channel_consistent_masking: false
  unmasked_channel_indices: null
  mask_value: 0
  pooling_type: "mean"
  head_dropout: 0.0
  num_parallel_samples: 100

  # rope positional embeddings
  max_wavelength: 500
  rope_percent: 0.75

  # channel embedding
  channel_embedding: "quadratic"
  polynomial_degree: 2

  pretrained_encoder_path: null

chronos:
  model_id: amazon/chronos-t5-small
  # model_id: google/t5-efficient-large
  model_type: seq2seq

  # model params
  random_init: false # NOTE: set to false for fine-tuning, very important!
  tie_embeddings: true

  context_length: 512
  prediction_length: 64
  num_samples: 20 # TODO: this is not actually being used. should we change it to match our number of samples?

  # vocab (tokens)
  n_tokens: 4096
  n_special_tokens: 2
  pad_token_id: 0
  eos_token_id: 1
  use_eos_token: true

  tokenizer_class: "MeanScaleUniformBins"
  tokenizer_kwargs:
    low_limit: -15.0
    high_limit: 15.0

  temperature: 1.0
  top_k: 50
  top_p: 1.0

