# PatchTST parameters
patchtst:
  mode: "predict"

  context_length: 512
  prediction_length: 128
  distribution_output: null #"student_t"
  loss: "mse"
  huber_delta: 1.0 # NOTE: only used if loss is Huber loss

  patch_length: 16
  patch_stride: 16
  num_hidden_layers: 8
  d_model: 512
  num_attention_heads: 8
  channel_attention: true # This mixes across channels in the encoder 
  ffn_dim: 512
  
  norm_type: "rmsnorm"
  norm_eps: 1e-05
  attention_dropout: 0.0
  positional_dropout: 0.0
  path_dropout: 0.0
  ff_dropout: 0.0
  bias: true
  activation_function: "gelu"
  pre_norm: true
  use_cls_token: false
  init_std: 0.02
  scaling: "std"
  do_mask_input: null
  mask_type: "random"
  random_mask_ratio: 0.5
  num_forecast_mask_patches: 3
  channel_consistent_masking: false
  unmasked_channel_indices: null
  mask_value: 0
  pooling_type: "max"
  head_dropout: 0.0
  num_parallel_samples: 100 # NOTE: only used in predict mode if distribution_output is not None

  # rope positional embeddings
  channel_rope: false  # use rope in channel attention
  max_wavelength: 500
  rope_percent: 0.75

  # for initializing the encoder from an mlm checkpoint
  pretrained_encoder_path: null

  # for initializing the prediction model from an existing prediction model checkpoint
  pretrained_pft_path: null

  # for the dynamical embedding
  use_dynamics_embedding: false
  num_poly_feats: 120
  poly_degrees: 2  # starting from 2 i.e. 2 means include 2nd and 3rd order terms
  rff_trainable: false
  rff_scale: 1.0
  num_rff: 256

  # ---- DeepSeek MoE Hyperparameters ----
  # 当 n_routed_experts > 0 时，将激活MoE层，否则使用标准的FFN
  n_routed_experts: 0             # 路由专家的总数 (例如: 8)
  num_experts_per_tok: 2          # 每个token要路由到的专家数量 (top-k, 例如: 2)
  
  # MoE专家网络内部的中间层维度。可以设置得比ffn_dim小，以节省参数
  moe_intermediate_size: 1024     
  
  # 共享专家的数量，通常为0或1。共享专家是一个所有token都会经过的标准FFN
  n_shared_experts: 1             
  
  # 负载均衡辅助损失的权重系数
  aux_loss_alpha: 0.00       
  
  # 是否将 top-k 专家的路由权重归一化为1
  norm_topk_prob: true


chronos:
  model_id: amazon/chronos-t5-mini
  model_type: seq2seq

  # model params
  random_init: false # NOTE: set to false for fine-tuning, very important!
  tie_embeddings: true

  context_length: 512
  prediction_length: 128
  num_samples: 20

  # vocab (tokens)
  n_tokens: 4096
  n_special_tokens: 2
  pad_token_id: 0
  eos_token_id: 1
  use_eos_token: true

  tokenizer_class: "MeanScaleUniformBins"
  tokenizer_kwargs:
    low_limit: -15.0
    high_limit: 15.0

  temperature: 1.0
  top_k: 50
  top_p: 1.0

