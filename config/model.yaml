# PatchTST parameters
patchtst:
  mode: "predict"

  # native patchtst config params
  num_input_channels: 1
  context_length: 512
  prediction_length: 64
  distribution_output: "student_t"
  loss: "mse"
  patch_length: 1
  patch_stride: 1
  num_hidden_layers: 3
  d_model: 128
  num_attention_heads: 4
  share_embedding: true
  channel_attention: false # This mixes across channels in the encoder 
  ffn_dim: 512
  norm_type: "layernorm"
  norm_eps: 1e-05
  attention_dropout: 0.0
  positional_dropout: 0.0
  path_dropout: 0.0
  ff_dropout: 0.0
  bias: true
  activation_function: "gelu"
  pre_norm: true
  positional_encoding_type: "sincos"
  use_cls_token: false
  init_std: 0.02
  share_projection: true  
  scaling: "std"
  do_mask_input: null
  mask_type: "random"
  random_mask_ratio: 0.5
  num_forecast_mask_patches: 3
  channel_consistent_masking: false
  unmasked_channel_indices: null
  mask_value: 0
  pooling_type: "mean"
  head_dropout: 0.0
  num_targets: 1
  output_range: null
  num_parallel_samples: 100

  # for the channel mixing head
  mix_channels: false
  mixing_d_model: 128
  mixing_num_attention_heads: 4
  mixing_attention_dropout: 0.0
  mixing_path_dropout: 0.0
  mixing_norm_eps: 1e-05
  mixing_ffn_dim: 512
  mixing_bias: true
  mixing_activation_function: "gelu"
  mixing_ff_dropout: 0.0
  mixing_pre_norm: true

chronos:
  model_id: amazon/chronos-t5-small
  # model_id: google/t5-efficient-large
  model_type: seq2seq

  # model params
  random_init: false # NOTE: set to false for fine-tuning, very important!
  tie_embeddings: true

  context_length: 512
  prediction_length: 64
  num_samples: 20 # TODO: this is not actually being used. should we change it to match our number of samples?

  # vocab (tokens)
  n_tokens: 4096
  n_special_tokens: 2
  pad_token_id: 0
  eos_token_id: 1
  use_eos_token: true

  tokenizer_class: "MeanScaleUniformBins"
  tokenizer_kwargs:
    low_limit: -15.0
    high_limit: 15.0

  temperature: 1.0
  top_k: 50
  top_p: 1.0

