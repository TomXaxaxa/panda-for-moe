# directory containing files used for training data
train_data_dir: "/stor/work/AMDG_Gilpin_Summer2024/data/train/"
# additional files to add to training data set
extra_train_data_paths: null
# - "/stor/work/AMDG_Gilpin_Summer2024/data/lorenz-data.arrow"
# probabilities to weight how many times we see data files
probability: null
# - 1.0
context_length: 512
prediction_length: 64
min_past: 60
max_steps: 2_000
save_steps: 2_000
log_steps: 100
per_device_train_batch_size: 32
learning_rate: 0.001
optim: adamw_torch_fused
num_samples: 20
shuffle_buffer_length: 2_000
gradient_accumulation_steps: 1
model_id: google/t5-efficient-small
model_type: seq2seq
random_init: false # NOTE: set to false for fine-tuning
tie_embeddings: true
output_dir: ./output/
tf32: false # NOTE: tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7
torch_compile: true
tokenizer_class: "MeanScaleUniformBins"
tokenizer_kwargs:
  low_limit: -15.0
  high_limit: 15.0
n_tokens: 4096
lr_scheduler_type: linear
warmup_ratio: 0.0
dataloader_num_workers: 1
max_missing_prop: 0.9
use_eos_token: true
